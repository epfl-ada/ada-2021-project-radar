{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-f3b61a84-dfd9-4c5a-8d0d-adc85d14773e",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Ada Final Project - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-0f001ec5-33aa-493f-a3b2-709e241074e5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27337,
    "execution_start": 1636627834187,
    "source_hash": "bf4cc646",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python Standard Libraries\n",
    "import re\n",
    "import csv\n",
    "import bz2\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Install using conda\n",
    "# conda install matplotlib pandas ipywidgets beautifulsoup4 nltk\n",
    "import nltk\n",
    "#import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Not available through Conda\n",
    "#!pip  install syllables transformers\n",
    "\n",
    "import syllables\n",
    "\n",
    "# BERT related modules\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download([\n",
    "     \"names\",\n",
    "     \"stopwords\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    " ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-ed300d7f-8297-46e9-b61e-0d90bdea681c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31,
    "execution_start": 1636627861542,
    "source_hash": "21f8699f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_FILE = 'data/hillary-and-trump-quotes-2016.json.bz2'\n",
    "\n",
    "CEFR_HTML_IN = \"data/cefr_data.html\"\n",
    "\n",
    "CEFR_CLEAN_CSV_IN = \"data/cefr_data_clean.csv\"\n",
    "CEFR_CSV_OUT = \"data/cefr_data.csv\"\n",
    "\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "RANDOM_SAMPLE_SIZE = 30_000\n",
    "\n",
    "SEED = 92813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-6564f599-70b8-46f2-9ecd-7c80c136de4b",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-1cd88084-afed-4a47-aad6-7c9a269eda38",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Initial Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-89611f62-5f85-4923-92ae-4dac4ffb8fda",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "For reference we include the code we executed on Google Colab to extract all quotes by **Hillary Clinton** and **Donald Trump** during the **year 2016** from the Quotebank dataset. This was a one time operation, which is why we did it outside of this notebook. All other algorithms we apply to the data will be/have been possibly iterated on for improvement, which is where the notebook format comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-6a758305-f895-4a95-bf2f-0d9056224eca",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```python\n",
    "PATH_TO_FILE = '/content/drive/MyDrive/Quotebank/quotes-2016.json.bz2'\n",
    "PATH_TO_OUT = '/content/drive/MyDrive/hillary-and-trump-quotes-2016.json.bz2'\n",
    "\n",
    "SPEAKER_NAMES = ['Hillary Clinton', 'Donald Trump']\n",
    "\n",
    "hits = 0\n",
    "\n",
    "with bz2.open(PATH_TO_FILE, 'rb') as s_file:\n",
    "    with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "\n",
    "        for instance in s_file:\n",
    "\n",
    "            instance = json.loads(instance)\n",
    "            speaker = instance['speaker']\n",
    "\n",
    "            if any(map(speaker.__contains__, desired_speakers)):\n",
    "\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "                hits += 1\n",
    "\n",
    "\n",
    "print(f\"Hits: {hits}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-d37921f2-e811-41fb-b687-d6cce16afa48",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29721,
    "execution_start": 1636627107359,
    "source_hash": "c87b5835",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_json(PATH_TO_FILE, lines=True, compression='bz2') #chunksize=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-0ed8fdb1-7821-47ee-8779-13b0e9ab7998",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 2. Enhance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-6f097d4a-86b9-4128-9c58-ede70e65f96b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 298,
    "execution_start": 1636627137124,
    "source_hash": "57723a31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2 = df_1\n",
    "\n",
    "df_2['proba'] = df_2['probas'].apply(lambda probas : float(probas[0][1]))\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-823ee243-60d7-489d-bdda-fe5b9e7e07f8",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-d933f3bb-8131-4599-af28-fc4cb32ea49c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 188,
    "execution_start": 1636627137429,
    "source_hash": "da860dd0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = df_2.sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)\n",
    "\n",
    "dt_sample = df_2[df_2['speaker'] == 'Donald Trump'].sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)\n",
    "hc_sample = df_2[df_2['speaker'] == 'Hillary Clinton'].sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-c0f1883f-0af7-450b-a573-35ca66048a3f",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 3. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-29c36457-8c23-4758-b6d6-36c47a66ead9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "We are only interested in quotes by **Hillary Clinton** and **Donald Trump** during the **year 2016**, specifically from 01/01/2016 - 01/01/2017. The subset loaded only contains the Quotebank quotes which have one (or both) of them as a possible speaker in the _speaker_ columns list and that lie in the specified time frame.\n",
    "\n",
    "Since the dataset was obtained using a ML model to extract and assign the quotes there will most likely be quotes which are faulty and quotes which have been assigned to the wrong speaker. The goal of the data cleaning is to remove such data points so that we can focus on working with as good data as possible.\n",
    "\n",
    "We must specify what makes a quote faulty and motivate this so that we remove as many bad quotes as possible while not removing any or as little actually correct quotes as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-65014d71-22c6-4993-9fb8-fc234790335a",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### 3.1 Filter: Remove quotes with low probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-c5ae7b51-3b86-4cfd-8da2-dcd71ebdcbec",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Motivation**\n",
    "\n",
    "Certain quotes that the model assigned to Trump and Clinton have very low probabilities to actually be quotes by them inside of the text as by the computation of the model. We want to learn about the distribution of the probability of the assigned quotes so that we can take a decision on if and when to filter out certain quotes due to a too low probability computed for them by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-02a52593-947c-4632-8e03-d03c2ce35993",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Distribution of the Proba**\n",
    "\n",
    "Here we plot the distribution of a sample of 1,000 Trump and 1,000 Clinton quotes respectively. The reason we sample seperatley is given the fact that we want to ensure a large enough sample pool for both candidates, which is needed since there are more quotes assigned to Trump than Clinton. We also plot the two seperatly to make sure that we do not miss any differences in the dsitribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-b6d4516b-815d-4609-9515-358ea7df0e1d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1291,
    "execution_start": 1636627137617,
    "source_hash": "627ca6e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_bins = 40\n",
    "proba_bins = [round((1 / n_bins), 2) * i for i in range(0, n_bins  + 1)]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"Distribution of the probability computed by the model to the assigned speaker\", fontsize=14)\n",
    "\n",
    "axs[0, 0].set(xlim=(0, 1))\n",
    "axs[0, 1].set(xlim=(0, 1))\n",
    "\n",
    "axs[0, 0].hist(hc_sample['proba'], bins=proba_bins, color='b', alpha=0.4)\n",
    "axs[0, 1].hist(dt_sample['proba'], bins=proba_bins, color='r', alpha=0.4)\n",
    "\n",
    "axs[1, 0].set(xlim=(0, 1))\n",
    "axs[1, 1].set(xlim=(0, 1))\n",
    "\n",
    "axs[1, 0].boxplot(hc_sample['proba'], vert=False)\n",
    "axs[1, 1].boxplot(dt_sample['proba'], vert=False)\n",
    "\n",
    "\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "axs[0, 0].title.set_text('Hillary Clinton')\n",
    "\n",
    "axs[0, 1].title.set_text('Donald Trump')\n",
    "\n",
    "axs[1, 1].set_xlabel('Computed Probability')\n",
    "axs[1, 0].set_xlabel('Computed Probability')\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print(\"Hillary Clinton Summary Statistics\")\n",
    "display(hc_sample['proba'].describe())\n",
    "print()\n",
    "print(\"Donald Trump Summary Statistics\")\n",
    "display(hc_sample['proba'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-daffbef2-bf68-4186-9edb-eb7e8219fac2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From plotting the distribution of probabilities which the model computed to the quotes it assigned to Trump and Hillary respectively we could now act in at least 3 ways:\n",
    "\n",
    "1. Remove any quote which is below the min probability minus a small margin because they are outliers. **I.e. Only Remove outliers.**\n",
    "\n",
    "2. Set the cutoff even higher because we decide to consider quotes with, ex. less than 0.4 probability assigned to the candidate being the speaker too weak to consider it in further analysis.\n",
    "\n",
    "3. A further possibility could be to remove quotes, where the next best speaker assigned has a similair/close probability compare to the number one.\n",
    "   \n",
    "   Example: `[ [ 'Trump', 0.41 ], [ 'Kanye West', 0.39 ], ...]`\n",
    "\n",
    "**TODO for later:** What should we do? What is scientifically sound? How do we motivate it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering out datapoints with too low probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-3464f846-1ed2-413f-80bb-129d46a1d96d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636627144403,
    "source_hash": "80bcd500",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROBAS_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_1 = df_3\n",
    "\n",
    "df_3_1 = df_3_1[df_3_1['proba'] >= PROBAS_THRESHOLD]\n",
    "\n",
    "n_removed_lines = len(df_3) - len(df_3_1)\n",
    "percentage_removed = (n_removed_lines / len(df_1)) * 100\n",
    "\n",
    "print(\"Removed {:,.0f} datapoints or {:,.2f}% of the original dataset\".format(n_removed_lines, percentage_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove quotes by different Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_1.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3_2 = df_3_1\n",
    "\n",
    "df_3_2 = df_3_2[df_3_2['speaker'].isin(['Hillary Clinton', 'Donald Trump'])]\n",
    "\n",
    "display(df_3_2.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation'])\n",
    "\n",
    "\n",
    "n_removed_lines = len(df_3_1) - len(df_3_2)\n",
    "percentage_removed = (n_removed_lines / len(df_1)) * 100\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Removed {:,.0f} datapoints or {:,.2f}% of the original dataset\".format(n_removed_lines, percentage_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-ae383ba4-e1d4-412e-a09b-1eb2e31ebf62",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### 3.3 Filter: Remove Quotes with \"nonsense\" content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a288c881-2a25-42c7-9a10-2df0459cbe9c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**TODO**: We should ideally check for the quality of the quotes. There are certainly some faulty quotes and maybe even gibberish in the dataset but it is hopefully very limited in scope. We should nonetheless attempt to look for faulty/gibbersih quotes which were extracted by the model and remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-92dc3725-32f5-480f-8f78-08064580b00a",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 4. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New sampling with the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sample = df_3_2.sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)\n",
    "\n",
    "dt_cleaned_sample = df_3_2[df_3_2['speaker'] == 'Donald Trump'].sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)\n",
    "hc_cleaned_sample = df_3_2[df_3_2['speaker'] == 'Hillary Clinton'].sample(n=RANDOM_SAMPLE_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patches for the Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_patch = mpatches.Patch(color='blue', alpha=0.4, label='Hillary Clinton')\n",
    "red_patch = mpatches.Patch(color='red', alpha=0.4, label='Donald Trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 General Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4_1 = df_3_2\n",
    "\n",
    "n_bins = 52\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.title(\"Histogram of all Quotes by H.C. and D.T during 2016\")\n",
    "\n",
    "df_4_1_hc = df_4_1[df_4_1['speaker'] == 'Hillary Clinton']\n",
    "df_4_1_dt = df_4_1[df_4_1['speaker'] == 'Donald Trump']\n",
    "\n",
    "plt.hist([df_4_1_hc['date'], df_4_1_dt['date']], weights=[df_4_1_hc['numOccurrences'], df_4_1_dt['numOccurrences']], bins=n_bins, color=[\"blue\", \"red\"], alpha=0.4)\n",
    "\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions arising**\n",
    "- Why are there certain periods with barely any quotes?\n",
    "- Is there something wrong/weird with the data?\n",
    "- How can we test that everything is good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Our Focus Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Media Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do media outlets portray Trump and Clinton differently? Do media outlets quote the two candidates equally much? Does the bias of the news outlet correlate with the quotes they report?\n",
    "\n",
    "Let's take two of the biggest outlets with political leaning views, CNN and Breitbart, and compare the distribution of Trump quotes with Clinton quotes, and also see how positive or negative they are.\n",
    "\n",
    "The NLTK library will also be used for sentiment analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sample.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Remove the wrong speakers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK’s Pre-Trained Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use NLTK VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "trump_polarity_scores = dt_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "dt_sample['polarityScore'] = [score.get('compound') for score in trump_polarity_scores]\n",
    "\n",
    "clinton_polarity_scores = hc_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "hc_sample['polarityScore'] = [score.get('compound') for score in clinton_polarity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df = dt_cleaned_sample\n",
    "clinton_df = hc_cleaned_sample\n",
    "\n",
    "cnn_trump, cnn_clinton = [],[]\n",
    "\n",
    "for i in range(len(trump_df)):\n",
    "    for k in range(len(trump_df['urls'].iloc[i])):\n",
    "        if trump_df['urls'].iloc[i][k].find('cnn') != -1:\n",
    "            cnn_trump.append(i)\n",
    "\n",
    "trump_cnn = trump_df.iloc[cnn_trump]\n",
    "\n",
    "for i in range(len(clinton_df)):\n",
    "    for k in range(len(clinton_df['urls'].iloc[i])):\n",
    "        if clinton_df['urls'].iloc[i][k].find('cnn') != -1:\n",
    "            cnn_clinton.append(i)\n",
    "\n",
    "clinton_cnn = clinton_df.iloc[cnn_clinton]\n",
    "        \n",
    "trump_cnn_PS = trump_cnn['quotation'].apply(sia.polarity_scores)\n",
    "trump_cnn['polarityScore'] = [score.get('compound') for score in trump_cnn_PS]\n",
    "clinton_cnn_PS = clinton_cnn['quotation'].apply(sia.polarity_scores)\n",
    "clinton_cnn['polarityScore'] = [score.get('compound') for score in clinton_cnn_PS]\n",
    "\n",
    "trump_cnn_pos = trump_cnn[trump_cnn['polarityScore'] >= 0]\n",
    "trump_cnn_neg = trump_cnn[trump_cnn['polarityScore'] < 0]\n",
    "clinton_cnn_pos = clinton_cnn[clinton_cnn['polarityScore'] >= 0]\n",
    "clinton_cnn_neg = clinton_cnn[clinton_cnn['polarityScore'] < 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].scatter(x=trump_cnn_pos['date'], y=trump_cnn_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "axs[0].scatter(x=trump_cnn_neg['date'], y=trump_cnn_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "axs[1].scatter(x=clinton_cnn_pos['date'], y=clinton_cnn_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "axs[1].scatter(x=clinton_cnn_neg['date'], y=clinton_cnn_neg['polarityScore'], color='r', alpha=0.3, label=\"positive\")\n",
    "\n",
    "fig.suptitle(\"CNN quotes compound polarity scores\", fontsize=14)\n",
    "\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].title.set_text('Donald Trump')\n",
    "\n",
    "axs[1].title.set_text('Hillary Clinton')\n",
    "\n",
    "axs[0].set_xlabel('Date')\n",
    "axs[1].set_xlabel('Date')\n",
    "\n",
    "axs[0].set_ylabel('Polarity score')\n",
    "axs[1].set_ylabel('Polarity score')\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart_trump, breitbart_clinton = [],[]\n",
    "\n",
    "for i in range(len(trump_df)):\n",
    "    for k in range(len(trump_df['urls'].iloc[i])):\n",
    "        if trump_df['urls'].iloc[i][k].find('breitbart') != -1:\n",
    "            breitbart_trump.append(i)\n",
    "\n",
    "trump_breitbart = trump_df.iloc[breitbart_trump]\n",
    "\n",
    "for i in range(len(clinton_df)):\n",
    "    for k in range(len(clinton_df['urls'].iloc[i])):\n",
    "        if clinton_df['urls'].iloc[i][k].find('breitbart') != -1:\n",
    "            breitbart_clinton.append(i)\n",
    "\n",
    "clinton_breitbart = clinton_df.iloc[breitbart_clinton]\n",
    "        \n",
    "trump_breitbart_PS = trump_breitbart['quotation'].apply(sia.polarity_scores)\n",
    "trump_breitbart['polarityScore'] = [score.get('compound') for score in trump_breitbart_PS]\n",
    "clinton_breitbart_PS = clinton_breitbart['quotation'].apply(sia.polarity_scores)\n",
    "clinton_breitbart['polarityScore'] = [score.get('compound') for score in clinton_breitbart_PS]\n",
    "\n",
    "trump_breitbart_pos = trump_breitbart[trump_breitbart['polarityScore'] >= 0]\n",
    "trump_breitbart_neg = trump_breitbart[trump_breitbart['polarityScore'] < 0]\n",
    "clinton_breitbart_pos = clinton_breitbart[clinton_breitbart['polarityScore'] >= 0]\n",
    "clinton_breitbart_neg = clinton_breitbart[clinton_breitbart['polarityScore'] < 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].scatter(x=trump_breitbart_pos['date'], y=trump_breitbart_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "axs[0].scatter(x=trump_breitbart_neg['date'], y=trump_breitbart_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "axs[1].scatter(x=clinton_breitbart_pos['date'], y=clinton_breitbart_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "axs[1].scatter(x=clinton_breitbart_neg['date'], y=clinton_breitbart_neg['polarityScore'], color='r', alpha=0.3, label=\"positive\")\n",
    "\n",
    "fig.suptitle(\"Breitbart quotes compound polarity scores\", fontsize=14)\n",
    "\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].title.set_text('Donald Trump')\n",
    "\n",
    "axs[1].title.set_text('Hillary Clinton')\n",
    "\n",
    "axs[0].set_xlabel('Date')\n",
    "axs[1].set_xlabel('Date')\n",
    "\n",
    "axs[0].set_ylabel('Polarity score')\n",
    "axs[1].set_ylabel('Polarity score')\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be a very small sample size for the EDA, but we can already see substantial bias both in terms of the number of quotes by a candidate in each media outlet as well as the ratio of positive/negative quotes chosen by the CNN and Breitbart to represent the two Presidential candidates. This most likely holds true for other popular media outlets, especially those with politically inclined views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Political Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to track the different political topics that the candidates focused on according to the content of their quotes. From this we aim to learn the importance of the topics in a absolut relative context but also in regards to when a certain topic might have been very present and then. disappeared for some time. We furthermore want to see if there might be a candidate which sparked a topic or at least started talking/being quoted about it first.\n",
    "\n",
    "Bewlow we show an example of a topic in regards to Obamacare, which we all remember to be a important topic during the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-07952c10-e99f-4cb2-8b2f-a25e0bf8546d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3516,
    "execution_start": 1636627350223,
    "source_hash": "fa48ee91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "OBAMACARE_REGEX = '(obama care)|obamacare' # Not perfect, proof of concept...\n",
    "\n",
    "df_obamacare = df_3_2[df_3_2['quotation'].str.contains(pat = OBAMACARE_REGEX, regex = True, flags=re.IGNORECASE)].sort_values('date')\n",
    "df_obamacare['date'] = df_obamacare['date'].apply(lambda date : date.date())\n",
    "\n",
    "df_obamacare_hc = df_obamacare[df_obamacare['speaker'] == 'Hillary Clinton']\n",
    "df_obamacare_dt = df_obamacare[df_obamacare['speaker'] == 'Donald Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-b295fa30-4d14-41c6-b300-892aaf1f6de3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4578,
    "execution_start": 1636627353740,
    "source_hash": "40817699",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BINS = 52\n",
    "ticks = [f\"2016-0{i * 3}-01\" for i in range(1,4)] + [\"2016-12-01\"]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Obamacare mentions in 2016\", fontsize=14)\n",
    "\n",
    "plt.hist([df_obamacare_hc['date'], df_obamacare_dt['date']], weights=[df_obamacare_hc['numOccurrences'], df_obamacare_dt['numOccurrences']], bins=BINS, color=[\"b\", \"r\"], alpha=0.4)\n",
    "\n",
    "plt.xticks(ticks)\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_topics_histogram(df_hc, df_dt, bins=52, topic_name=\"\"):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    kwargs = {\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\": bins,\n",
    "    }\n",
    "\n",
    "    axs[0].hist(df_hc['date'], weights=df_hc['numOccurrences'], color=\"b\", **kwargs)\n",
    "    axs[1].hist(df_dt['date'], weights=df_dt['numOccurrences'], color=\"r\", **kwargs)\n",
    "\n",
    "    fig.suptitle(f\"Mentions of the Topic '{topic_name}' in 2016 by candidate\", fontsize=16)\n",
    "\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "\n",
    "    axs[0].title.set_text('Hillary Clinton')\n",
    "    axs[0].set_xlabel('Date')\n",
    "\n",
    "    axs[1].title.set_text('Donald Trump')\n",
    "    axs[1].set_xlabel('Date')\n",
    "    \n",
    "    ticks = [f\"2016-0{i * 3}-01\" for i in range(1,4)] + [\"2016-12-01\"]\n",
    "\n",
    "    axs[0].set_xticks(ticks)\n",
    "    axs[1].set_xticks(ticks)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "quote_topics_histogram(df_obamacare_hc, df_obamacare_dt, topic_name=\"Obamacare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Interrlude\n",
    "\n",
    "Hillary Clinton talked less about Obamacare but instead more about Healthcare... To be explored further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation Idea\n",
    "\n",
    "We have identified several topics which show a clear periodic difference in occurance and intensity. We aim to visualise this data as illustrated in the sketch below for the final project. We hope to show how several topics come and go throughout the campaign and are focused on with a different amount of attention/importance depending on the candidate. Ideally we would extend this visualisation further allowing the viewer to see the importance the citizen base gave the topics at each time or the extent to which media reported on it. It would be amazing to see if some topics had a trendsetter who started talking about it earier adn fiercly...\n",
    "\n",
    "We believe that our first EDA supports our thesis that the data will allow us to visualise it in this way, that there is peridoicity and hoepefully many interesting insights.\n",
    "\n",
    "![Visualisation Sketch](focus-topics-sketch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-89407345-46f3-4822-af79-aa219e1c362b",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Q3: Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-872c0020-2014-496f-b174-2a714fe55dcb",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### a) Intellectuality of Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-ba5fd85d-d420-4cf7-aca7-c62741f0ee50",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Idea 1: Count the syllables in each candidate's consolidated quotes and compare distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-ff68b023-ba41-43aa-8e52-2ba0328600e9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1636627516845,
    "source_hash": "e1eb850e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def syllables_hist(dt_text: str, hc_text: str):\n",
    "\n",
    "    dt_syl = list(map(syllables.estimate, dt_text.split()))\n",
    "    hc_syl = list(map(syllables.estimate, hc_text.split()))\n",
    "\n",
    "    kwargs = {\n",
    "        \"x\"    : [hc_syl, dt_syl],\n",
    "        \"label\" : [\"Hillary Clinton\", \"Donald Trump\"],\n",
    "        \"color\" : [\"b\", \"r\"],\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\" : range(1, 6),\n",
    "        \"density\" : True,\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "\n",
    "    plt.hist(**kwargs)\n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram of number of syllables\", fontsize=16)\n",
    "    plt.xlabel(\"Number of syllables in a Word\")\n",
    "    plt.ylabel(\"Probability of Occuring\")\n",
    "\n",
    "    # TODO generalize this\n",
    "    plt.xticks(range(1, 5), range(1, 5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-53a7c04e-35a8-4276-a1ef-5c1d6fe205f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1636627518342,
    "source_hash": "9ad549ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc_text = ' '.join(hc_cleaned_sample[\"quotation\"])\n",
    "dt_text = ' '.join(dt_cleaned_sample[\"quotation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-8be22ba2-c920-46eb-aabd-da3c4954f2f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4005,
    "execution_start": 1636627520438,
    "source_hash": "8e9bff90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "syllables_hist(dt_text, hc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-5efdf486-dff7-40c2-9c6c-0636b6a629cf",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Idea 2: Look at the CEFR language level. We can measure the language level for all words in each candidate's consolidated quotes and compare the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-dd63e800-673b-42f3-a2f9-57c9c8b8b230",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "But first we need to get data on words. We have the HTML for a web page containing these data, and we can put them in the form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-5a090841-8650-42e0-bce0-e08fe18a1ddb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636627750559,
    "source_hash": "bb24539e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bs_to_csv(soup, csv_writer):\n",
    "    table = soup.find(\"tbody\")\n",
    "    tr_rows = table.find_all(\"tr\")\n",
    "    csv_writer.writerow([\"word\", \"guideword\", \"level\", \"part of speech\", \"topic\"])\n",
    "    csv_writer.writerows(\n",
    "        [[t.text for t in r.find_all(\"td\")][:-1] for r in tr_rows]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-761bac2b-b461-43cf-8630-f6a3811e3088",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25741,
    "execution_start": 1636627762910,
    "source_hash": "a020fbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape the HTML file and write in CSV format to file.\n",
    "with open(CEFR_HTML_IN) as fp, open(CEFR_CSV_OUT, 'w') as csv_out:\n",
    "    soup = BeautifulSoup(fp)\n",
    "    csv_writer = csv.writer(csv_out, delimiter=',')\n",
    "    bs_to_csv(soup, csv_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-9a1ff974-a9b3-4bf4-a867-b1519bc3f6eb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 283,
    "execution_start": 1636627800203,
    "source_hash": "11315500",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr = pd.read_csv(CEFR_CSV_OUT)\n",
    "\n",
    "display(df_cefr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-4fd8864f-f336-4da5-bdc8-bc782b7f6a43",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The data includes idioms as well as nouns; this is intelligible for a human, but it might be a bit of trouble to make it work here, so we remove them.\n",
    "We'll only need the `word` and `level` columns, so we remove the others as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-250974a1-a51d-44df-b15e-36b1f85972b2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627822556,
    "source_hash": "7d9ff058",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr = df_cefr[[len(w.split()) == 1 for w in df_cefr[\"word\"]]]\\\n",
    "                 .filter(items=[\"word\", \"level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00042-0cd59360-8668-472a-ba85-e0099da5bb22",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we set everything to lowercase. Also it seems that some words have punctuation surrounding them, which is not desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-058ad49b-70ac-46aa-9492-04b0e59359d1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 165,
    "execution_start": 1636627861574,
    "source_hash": "4515b720",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr[\"word\"] = df_cefr[\"word\"].transform(\n",
    "    lambda w: w.lower()\\\n",
    "               .translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "display(df_cefr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-2ffff90e-01d2-4cda-94af-77e2af2591a8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Also, the same word can appear multiple times (as there might be different phrases in which the word means something slightly different) so we aggregate the levels using the median level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-72d203e6-dc7e-496e-8593-b9569fec7518",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636627870310,
    "source_hash": "316ad935",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_level_map = {\n",
    "    \"A1\": 1,\n",
    "    \"A2\": 2,\n",
    "    \"B1\": 3,\n",
    "    \"B2\": 4,\n",
    "    \"C1\": 5,\n",
    "    \"C2\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-36bcb5df-a186-429d-bf1d-18f6c35a39b6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627870310,
    "source_hash": "c27cea23",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr_copy = df_cefr.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-81ab8fa7-fab1-498c-a201-42e5f292a658",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6245357,
    "execution_start": 1636627878602,
    "source_hash": "8e05a8b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the map to transform the level column\n",
    "\n",
    "df_cefr.level = df_cefr_copy[\"level\"].map(cefr_level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-63bcb8c7-7a36-4104-97f4-87d62a161869",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 49,
    "execution_start": 1636627878606,
    "source_hash": "467c10dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate with the median\n",
    "\n",
    "df_cefr = df_cefr.groupby(\"word\").agg(\"median\").reset_index()\n",
    "\n",
    "display(df_cefr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-58c7cde6-410a-4e31-946c-9402e366c510",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Finally we change the index to the word itself to facilitate searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-018ebaca-0490-47d0-ad3a-42c9a85fcd47",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1636627887076,
    "source_hash": "fc853559",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr.index = df_cefr.word\n",
    "df_cefr = df_cefr.filter(items=[\"level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-e8e262d5-218b-48df-901a-82b3969937ee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we can finally apply this to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-e4c312e2-ae60-48e5-9abd-6145d5f8c347",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1079,
    "execution_start": 1636627895974,
    "source_hash": "819e084f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_data = pd.read_csv(CEFR_CLEAN_CSV_IN, index_col=\"word\")\n",
    "cefr_data.loc[\"he\"].loc[\"level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-c34a4a1e-f0ed-4ae3-a3db-51ee758d95ea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627897052,
    "source_hash": "3be90380",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cefr_level_hist(dt_text: str, hc_text: str):\n",
    "\n",
    "    cefr_data = pd.read_csv(CEFR_CLEAN_CSV_IN, index_col=\"word\")\n",
    "    cefr_data.loc[\"he\"].loc[\"level\"]\n",
    "    \n",
    "    level = lambda w: cefr_data.loc[w].loc[\"level\"] if w in cefr_data.index else 0\n",
    "\n",
    "    dt_level = list(map(level, dt_text.split()))\n",
    "    hc_level = list(map(level, hc_text.split()))\n",
    "\n",
    "    kwargs = {\n",
    "        \"x\"    : [hc_level, dt_level],\n",
    "        \"label\" : [\"Hillary Clinton\", \"Donald Trump\"],\n",
    "        \"color\" : [\"b\", \"r\"],\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\" : range(0, 7),\n",
    "        \"density\" : True,\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    plt.hist(**kwargs)\n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram of English level of words\", fontsize=16)\n",
    "    plt.xlabel(\"English level\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "    plt.xticks(range(0, 7), [\"NA\", \"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-76a016f3-dcbd-47e9-b61d-aad52cc9dabc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8380,
    "execution_start": 1636627988679,
    "source_hash": "320fd282",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_level_hist(dt_text.lower(), hc_text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-2010d14a-5465-46c1-9a07-70de3a7b317d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There seems to be little difference in terms of language levels, but note how many words were not accounted for; this is due in part to limitations of our CEFR dataset, and also to the fact that some words appear in text in different forms (e.g. a verb conjugated to the past tense). This particular issue will be remedied by \"stemming\" the words to reduce them to their \"base\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-e516fc39-2203-4228-a596-a70c710fcdff",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-641a5cca-81ab-414f-a4d9-a15f612ec7d0",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "- Who is are Trump and Clinton most confused with? Using the other speaker attributions in the quotas list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-d81c6fba-39b1-4038-b276-31424bf154d8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### b) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-7901461f-b123-4ddd-93b0-8b7a56ba60df",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1636628014104,
    "source_hash": "6be9ac84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_sample.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-1d80104a-8fbb-45d5-869b-9acc243f5c73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "##### Using NLTK’s Pre-Trained Sentiment Analyzer\n",
    "We use NLTK VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-8c09ef1c-e0b0-4b8c-bb12-941f21af8fea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636628096363,
    "source_hash": "c4301c8e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-b14c7f35-80d4-434d-bcdb-0b19407b08bc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1117,
    "execution_start": 1636628104875,
    "source_hash": "297ff81f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_polarity_scores = dt_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "dt_cleaned_sample['polarityScore'] = [score.get('compound') for score in trump_polarity_scores]\n",
    "\n",
    "clinton_polarity_scores = hc_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "hc_cleaned_sample['polarityScore'] = [score.get('compound') for score in clinton_polarity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topk_quotes(df, sentiment, k):\n",
    "    if (sentiment == \"positive\"):\n",
    "        [print(\"\\n[+]\", quote) for quote in df.sort_values('polarityScore', ascending=False)['quotation'].head(k).tolist()]\n",
    "    else:\n",
    "        [print(\"\\n[-]\", quote) for quote in df.sort_values('polarityScore', ascending=True)['quotation'].head(k).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-8a5144b1-48ed-4db9-bd4d-b2aec6dda51d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Donald Trump's 5 most positive and negative quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-c8eac542-3bcf-492b-b29f-f9008f466a92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1636628114523,
    "source_hash": "da136360",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Trump's 5 most positive quotes:\")\n",
    "# trump_df.sort_values('polarityScore', ascending=False)[['quotation', 'polarityScore']].head(5)\n",
    "[print(\"\\n[+]\", quote) for quote in dt_cleaned_sample.sort_values('polarityScore', ascending=False)['quotation'].head(5).tolist()]\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"Trump's 5 most negative quotes:\")\n",
    "# trump_df.sort_values('polarityScore', ascending=True)[['quotation', 'polarityScore']].head(5).tolist()\n",
    "[print(\"\\n[-]\", quote) for quote in dt_cleaned_sample.sort_values('polarityScore', ascending=True)['quotation'].head(5).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00043-2f3e0e22-945f-422b-b7ae-50449805d62a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Hillary Clinton's 5 most positive and negative quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-5c0acf26-1036-4ef7-ad8d-769ab34a37e6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39,
    "execution_start": 1636628122215,
    "source_hash": "dd5a4f6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Hillary Clinton's 5 most positive quotes:\")\n",
    "# clinton_df.sort_values('polarityScore', ascending=False)[['quotation', 'polarityScore']].head(5)\n",
    "[print(\"\\n[+]\", quote) for quote in hc_cleaned_sample.sort_values('polarityScore', ascending=False)['quotation'].head(5).tolist()]\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"Hillary Clinton's 5 most negative quotes:\")\n",
    "# clinton_df.sort_values('polarityScore', ascending=True)[['quotation', 'polarityScore']].head(5).tolist()\n",
    "[print(\"\\n[-]\", quote) for quote in hc_cleaned_sample.sort_values('polarityScore', ascending=True)['quotation'].head(5).tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-a67380e5-aba9-41bc-9b85-1e2eb5609c87",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 889,
    "execution_start": 1636628130647,
    "source_hash": "d0db659a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_df_pos = dt_sample[dt_sample['polarityScore'] >= 0]\n",
    "trump_df_neg = dt_sample[dt_sample['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_df_pos['date'], y=trump_df_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_df_neg['date'], y=trump_df_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-98198884-dda8-47de-9e2b-678e1574e10d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 702,
    "execution_start": 1636628140656,
    "source_hash": "3b4cb853",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinton_df_pos = hc_sample[hc_sample['polarityScore'] >= 0]\n",
    "clinton_df_neg = hc_sample[hc_sample['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=clinton_df_pos['date'], y=clinton_df_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=clinton_df_neg['date'], y=clinton_df_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Clinton's quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Hillary Clinton and Donald Trumps compound sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with matplotlib\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6),  sharey=True, sharex=True)\n",
    "\n",
    "bins = 40 \n",
    "kwargs = {\n",
    "    \"alpha\" : 0.4,\n",
    "    \"bins\": bins,\n",
    "}\n",
    "    \n",
    "# clinton compound polarity distribution\n",
    "ax[0].hist(hc_sample['polarityScore'], color=\"b\", **kwargs)\n",
    "ax[0].set_title(\"Hilary Clinton\")\n",
    "ax[0].set_xlabel('Compound sentiment')\n",
    "ax[0].set_ylabel('Number of quotes')\n",
    "\n",
    "# trump compound polarity distribution\n",
    "ax[1].hist(dt_sample['polarityScore'], color=\"r\", **kwargs)\n",
    "ax[1].set_title(\"Donald Trump\")\n",
    "ax[1].set_xlabel('Compound sentiment')\n",
    "# ax[1].set_ylabel('Number of quotes')\n",
    "    \n",
    "fig.suptitle(\"Hilary Clinton VS Donald Trump quotes compound polarity distribution in 2016\", fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with plotly\n",
    "hc_dt_sample = pd.DataFrame(dict(\n",
    "    candidate = np.concatenate(([\"Hillary Clinton\"]*len(hc_sample), \n",
    "                             [\"Donald Trump\"]*len(dt_sample))), \n",
    "    polarityScore   = np.concatenate((hc_sample['polarityScore'],\n",
    "                                      dt_sample['polarityScore']))\n",
    "))\n",
    "\n",
    "fig = px.histogram(hc_dt_sample, x=\"polarityScore\", color=\"candidate\", \n",
    "                   barmode=\"overlay\", nbins=40, marginal=\"box\",\n",
    "                   width=750, height=500,\n",
    "                   log_y=True\n",
    ")\n",
    "\n",
    "# customize font and legend orientation & position\n",
    "fig.update_layout( \n",
    "    font_family=\"Rockwell\",\n",
    "    legend=dict(\n",
    "        title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "    ),\n",
    "    title_text='Hilary Clinton VS Donald Trump quotes compound polarity distribution in 2016', # title of plot\n",
    "    xaxis_title_text='Sentiment (polarity compound score)',\n",
    "    yaxis_title_text='Number of quotes (log scale)', \n",
    "    # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.05 # gap between bars of the same location coordinates\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of our conditions\n",
    "sentiment_conditions = [\n",
    "    (hc_dt_sample['polarityScore'] <= -0.05),\n",
    "    (np.abs(hc_dt_sample['polarityScore']) < 0.05),\n",
    "    (hc_dt_sample['polarityScore'] >= 0.05)\n",
    "]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "hc_dt_sample['sentiment'] = np.select(sentiment_conditions, values)\n",
    "\n",
    "# display updated DataFrame\n",
    "# display(hc_dt_sample.head())\n",
    "\n",
    "# aggregate our dataframe by sentiment and candidate\n",
    "df_sentiment_agg = hc_dt_sample.groupby(['sentiment', 'candidate']).size().reset_index(name='counts')\n",
    "\n",
    "# sort to have Clinton values first\n",
    "df_sentiment_agg.sort_values('candidate', ascending=False, inplace=True)\n",
    "\n",
    "fig = px.bar(df_sentiment_agg, x=\"sentiment\", y=\"counts\",\n",
    "             color='candidate', barmode='group',\n",
    "             color_discrete_map={\n",
    "                'Hillary Clinton': px.colors.qualitative.Plotly[0],\n",
    "                'Donald Trump': px.colors.qualitative.Plotly[1],\n",
    "             },\n",
    "             height=400,\n",
    "            text=\"counts\"\n",
    "            )\n",
    "\n",
    "# customize font and legend orientation & position\n",
    "fig.update_layout( \n",
    "    font_family=\"Rockwell\",\n",
    "    legend=dict(\n",
    "        title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "    ),\n",
    "    title_text='Overall Sentiment of Clinton vs Trump quotes in 2016', title_x=0.5,\n",
    "    xaxis_title_text='Sentiment',\n",
    "    yaxis_title_text='Number of quotes', \n",
    "    # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    # bargroupgap=0.05 # gap between bars of the same location coordinates\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n(Negative score < -0.05, Neutral score: -0.05 < score < 0.05, Positive score > 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate totals\n",
    "dt_tot_quotes_count = len(dt_sample)\n",
    "dt_pos_quotes_count = sum(dt_sample['polarityScore']>=0.05)\n",
    "dt_neg_quotes_count = sum(dt_sample['polarityScore']<=-0.05)\n",
    "dt_neu_quotes_count = sum(np.abs(dt_sample['polarityScore'])<0.05)\n",
    "\n",
    "hc_tot_quotes_count = len(hc_sample)\n",
    "hc_pos_quotes_count = sum(hc_sample['polarityScore']>=0.05)\n",
    "hc_neg_quotes_count = sum(hc_sample['polarityScore']<=-0.05)\n",
    "hc_neu_quotes_count = sum(np.abs(hc_sample['polarityScore'])<0.05)\n",
    "\n",
    "# dt_pos_quotes_count = df_sentiment_agg['counts'].loc[(df_sentiment_agg['candidate'] == 'Donald Trump') & (df_sentiment_agg['sentiment'] == 'positive')]\n",
    "# dt_neg_quotes_count = df_sentiment_agg['counts'].loc[(df_sentiment_agg['candidate'] == 'Donald Trump') & (df_sentiment_agg['sentiment'] == 'negative')]\n",
    "# dt_neu_quotes_count = df_sentiment_agg['counts'].loc[(df_sentiment_agg['candidate'] == 'Donald Trump') & (df_sentiment_agg['sentiment'] == 'neutral')]\n",
    "# dt_tot_quotes_count = dt_pos_quotes_count + dt_neg_quotes_count + dt_neu_quotes_count\n",
    "\n",
    "# hc_pos_quotes_count = df_sentiment_agg.loc[(df_sentiment_agg['candidate'] == 'Hillary Clinton') & (df_sentiment_agg['sentiment'] == 'positive')]\n",
    "# hc_neg_quotes_count = df_sentiment_agg.loc[(df_sentiment_agg['candidate'] == 'Hillary Clinton') & (df_sentiment_agg['sentiment'] == 'negative')]\n",
    "# hc_neu_quotes_count = df_sentiment_agg.loc[(df_sentiment_agg['candidate'] == 'Hillary Clinton') & (df_sentiment_agg['sentiment'] == 'neutral')]\n",
    "# hc_tot_quotes_count = hc_pos_quotes_count + hc_neg_quotes_count + hc_neu_quotes_count\n",
    "\n",
    "\n",
    "# summary\n",
    "print()\n",
    "print('                                     Hilary Clinton     |      Donald Trump      ')\n",
    "print('                                ------------------------|------------------------')\n",
    "print('Total number quotes in sample:        {:>4} ({:.1%})     |      {:>4} ({:.1%})'.format(hc_tot_quotes_count, hc_tot_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                               dt_tot_quotes_count, dt_tot_quotes_count/dt_tot_quotes_count))\n",
    "print('Number of negative quotes:            {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_neg_quotes_count, hc_neg_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_neg_quotes_count, dt_neg_quotes_count/dt_tot_quotes_count ))\n",
    "print('Number of neutral quotes:             {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_neu_quotes_count, hc_neu_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_neu_quotes_count, dt_neu_quotes_count/dt_tot_quotes_count))\n",
    "print('Number of positive quotes:            {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_pos_quotes_count, hc_pos_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_pos_quotes_count, dt_pos_quotes_count/dt_tot_quotes_count ))\n",
    "\n",
    "print(\"\\n(Positive score > 0.05, Negative score < -0.05, Neutral score: -0.05 < score < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords):\n",
    "    hc_filtered = hc_sample[hc_sample['quotation'].str.contains(topic_keywords, case=False)]\n",
    "    dt_filtered = dt_sample[dt_sample['quotation'].str.contains(topic_keywords, case=False)]\n",
    "    \n",
    "    hc_dt_filtered = pd.DataFrame(dict(\n",
    "        candidate = np.concatenate(([\"Hillary Clinton\"]*len(hc_filtered), \n",
    "                                 [\"Donald Trump\"]*len(dt_filtered))), \n",
    "        polarityScore   = np.concatenate((hc_filtered['polarityScore'],\n",
    "                                          dt_filtered['polarityScore'])),\n",
    "        quotation   = np.concatenate((hc_filtered['quotation'],\n",
    "                                          dt_filtered['quotation']))\n",
    "    ))\n",
    "    return hc_dt_filtered    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentHistogram(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "    \n",
    "    # plot with plotly\n",
    "    fig = px.histogram(hc_dt_filtered, x=\"polarityScore\", color=\"candidate\", \n",
    "                       barmode=\"overlay\", nbins=40, marginal=\"box\",\n",
    "                       width=750, height=500,\n",
    "                       # log_y=True\n",
    "    )\n",
    "\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Sentiment anlysis on topic: '+topic_keywords, title_x=0.5,\n",
    "        xaxis_title_text='Sentiment (polarity compound score)',\n",
    "        yaxis_title_text='Number of quotes (log scale)', \n",
    "        # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "        bargroupgap=0.05, # gap between bars of the same location coordinates\n",
    "    )\n",
    "    fig.update_xaxes(range=[-1, 1])\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidateBarChart(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "    quote_counts = hc_dt_filtered.groupby(['candidate']).size().reset_index(name='counts')\n",
    "    hc_quote_counts = quote_counts[quote_counts['candidate'] == 'Hillary Clinton'].counts\n",
    "    dt_quote_counts = quote_counts[quote_counts['candidate'] == 'Donald Trump'].counts\n",
    "\n",
    "    fig = px.bar(quote_counts, x=\"candidate\", y=\"counts\",\n",
    "                 color='candidate',\n",
    "                 color_discrete_map={\n",
    "                    'Hillary Clinton': px.colors.qualitative.Plotly[0],\n",
    "                    'Donald Trump': px.colors.qualitative.Plotly[1],\n",
    "                 },\n",
    "                 height=400,\n",
    "                text=\"counts\"\n",
    "                )\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Total number of quotes about :'+topic_keywords, title_x=0.5,\n",
    "        xaxis_title_text='Candidate',\n",
    "        yaxis_title_text='Number of quotes', \n",
    "        # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "        # bargroupgap=0.05 # gap between bars of the same location coordinates\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentBarChart(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "    # create a list of our conditions\n",
    "    sentiment_conditions = [\n",
    "        (hc_dt_filtered['polarityScore'] <= -0.05),\n",
    "        (np.abs(hc_dt_filtered['polarityScore']) < 0.05),\n",
    "        (hc_dt_filtered['polarityScore'] >= 0.05)\n",
    "    ]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    hc_dt_filtered['sentiment'] = np.select(sentiment_conditions, values)\n",
    "\n",
    "    # aggregate our dataframe by sentiment and candidate\n",
    "    df_sentiment_agg = hc_dt_filtered.groupby(['sentiment', 'candidate']).size().reset_index(name='counts')\n",
    "\n",
    "    # NORMALIZATION\n",
    "\n",
    "    # compute total number of quotes per candidate\n",
    "    # quote_counts = hc_dt_filtered.groupby(['candidate']).size()\n",
    "    # hc_quote_counts = quote_counts['Hillary Clinton']\n",
    "    # dt_quote_counts = quote_counts['Donald Trump']\n",
    "\n",
    "    quote_counts = hc_dt_filtered.groupby(['candidate']).size().reset_index(name='counts')\n",
    "    hc_quote_counts = quote_counts[quote_counts['candidate'] == 'Hillary Clinton'].counts\n",
    "    dt_quote_counts = quote_counts[quote_counts['candidate'] == 'Donald Trump'].counts\n",
    "\n",
    "    hc_quote_counts = hc_quote_counts.values[0]\n",
    "    dt_quote_counts = dt_quote_counts.values[0]\n",
    "\n",
    "    # divide by total number of quotes per candidate to get percentage\n",
    "    df_sentiment_agg['counts_pct'] = 0\n",
    "    df_sentiment_agg['counts_pct'] = np.where(df_sentiment_agg['candidate'] == 'Donald Trump', df_sentiment_agg['counts']/dt_quote_counts, df_sentiment_agg['counts_pct'])\n",
    "    df_sentiment_agg['counts_pct'] = np.where(df_sentiment_agg['candidate'] == 'Hillary Clinton', df_sentiment_agg['counts']/hc_quote_counts, df_sentiment_agg['counts_pct'])\n",
    "\n",
    "    # sort to have Clinton values first\n",
    "    df_sentiment_agg.sort_values('candidate', ascending=False, inplace=True)\n",
    "\n",
    "    fig = px.bar(df_sentiment_agg, x=\"sentiment\", y=\"counts_pct\",\n",
    "                 color='candidate', barmode='group',\n",
    "                 color_discrete_map={\n",
    "                    'Hillary Clinton': px.colors.qualitative.Plotly[0],\n",
    "                    'Donald Trump': px.colors.qualitative.Plotly[1],\n",
    "                 },\n",
    "                 height=400,\n",
    "                text=\"counts_pct\"\n",
    "                )\n",
    "\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Overall Sentiment of Clinton vs Trump about: '+topic_keywords, title_x=0.5,\n",
    "        xaxis_title_text='Sentiment',\n",
    "        yaxis_title_text='% of quotes per candidate', \n",
    "        # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "        # bargroupgap=0.05 # gap between bars of the same location coordinates\n",
    "    )\n",
    "    fig.update_traces(texttemplate='%{text:.0%}', textposition='inside')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    print(\"\\n(Negative score < -0.05, Neutral score: -0.05 < score < 0.05, Positive score > 0.05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'economy'\n",
    "# sentimentHistogram(hc_sample, dt_sample, topic_keywords)\n",
    "candidateBarChart(hc_sample, dt_sample, topic_keywords)\n",
    "sentimentBarChart(hc_sample, dt_sample, topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'immigration|immigrant|immigrants|border'\n",
    "# sentimentHistogram(hc_sample, dt_sample, topic_keywords)\n",
    "sentimentBarChart(hc_sample, dt_sample, topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'obamacare'\n",
    "# sentimentHistogram(hc_sample, dt_sample, topic_keywords)\n",
    "sentimentBarChart(hc_sample, dt_sample, topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'abortion'\n",
    "# sentimentHistogram(hc_sample, dt_sample, topic_keywords)\n",
    "sentimentBarChart(hc_sample, dt_sample, topic_keywords)\n",
    "hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "print_topk_quotes(hc_dt_filtered, \"positive\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00067-4060b568-755a-4b12-b428-8ef964b364a6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Sentiment analysis on the target of quotes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, Trump calling Clinton \"Crooked Hillary\" would be a negative statement about Presidential candidate Clinton. This will be done for a number of politicians, including: Hillary Clinton, Nancy Pelosi, Barack Obama, Bernie Sanders, Elizabeth Warren, Ted Cruz, Mike Pence, and Mitch McConnell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00068-630091d6-988f-4d0f-b56f-ebdaa90a1ad8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1636628202255,
    "source_hash": "7ab19ee2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_df = sample.loc[sample['speaker'] == 'Donald Trump']\n",
    "clinton_df = sample.loc[sample['speaker'] == 'Hillary Clinton']\n",
    "\n",
    "trump_on_clinton = trump_df[trump_df['quotation'].str.contains('clinton|hillary', case=False)]\n",
    "trump_on_obama = trump_df[trump_df['quotation'].str.contains('obama|barack', case=False)]\n",
    "trump_on_sanders = trump_df[trump_df['quotation'].str.contains('bernie|sanders', case=False)]\n",
    "\n",
    "clinton_on_trump = clinton_df[clinton_df['quotation'].str.contains('trump|donald', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00069-c34cd4a1-af5a-49a4-8152-7de63b2f3811",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 71,
    "execution_start": 1636628211016,
    "source_hash": "952ab1a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_clinton_PS = trump_on_clinton['quotation'].apply(sia.polarity_scores)\n",
    "trump_on_clinton['polarityScore'] = [score.get('compound') for score in trump_on_clinton_PS]\n",
    "\n",
    "trump_on_obama_PS = trump_on_obama['quotation'].apply(sia.polarity_scores)\n",
    "trump_on_obama['polarityScore'] = [score.get('compound') for score in trump_on_obama_PS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00070-17faff10-d143-4775-bf17-92941d690a18",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 786,
    "execution_start": 1636628233584,
    "source_hash": "a6243f79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_clinton_pos = trump_on_clinton[trump_on_clinton['polarityScore'] >= 0]\n",
    "trump_on_clinton_neg = trump_on_clinton[trump_on_clinton['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_on_clinton_pos['date'], y=trump_on_clinton_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_on_clinton_neg['date'], y=trump_on_clinton_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's Hillary-targeted quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00071-28c2b03b-740e-48bc-8007-0e938deb775c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 713,
    "execution_start": 1636628288940,
    "source_hash": "28ea1bcd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_obama_pos = trump_on_obama[trump_on_obama['polarityScore'] >= 0]\n",
    "trump_on_obama_neg = trump_on_obama[trump_on_obama['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_on_obama_pos['date'], y=trump_on_obama_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_on_obama_neg['date'], y=trump_on_obama_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's Obama-targeted quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00072-23bb561b-cc74-4903-b22e-f035b6895e8f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Perhaps a next step would be to compare this with a baseline ratio of positive/negative words. For instance, maybe on average people say 70% positive things and 30% negative. Knowing this could help compare results with Trump and Clinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT for sentiment analysis using quote vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['quotation'].iloc[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the special tokens.\n",
    "trump_df['marked'] = \"[CLS] \" + trump_df['quotation'] + \" [SEP]\"\n",
    "\n",
    "trump_df['tokenized'] = trump_df['marked']\n",
    "trump_df['indexed'] = trump_df['marked']\n",
    "trump_df['seg_ids'] = trump_df['marked']\n",
    "trump_df['tokens_tensor'] = trump_df['marked']\n",
    "trump_df['seg_tensors'] = trump_df['marked']\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "for i in range(len(trump_df)):\n",
    "    trump_df['tokenized'].iloc[i] = tokenizer.tokenize(trump_df['marked'].iloc[i])\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    trump_df['indexed'].iloc[i] = tokenizer.convert_tokens_to_ids(trump_df['tokenized'].iloc[i])\n",
    "    trump_df['seg_ids'].iloc[i] = [1] * len(trump_df['tokenized'].iloc[i])\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    trump_df['tokens_tensor'].iloc[i] = torch.tensor([trump_df['indexed'].iloc[i]])\n",
    "    trump_df['seg_tensors'].iloc[i] = torch.tensor([trump_df['seg_ids'].iloc[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Next Steps\n",
    "\n",
    "The next step is to evaluate BERT on our quotations, and fetch the hidden states of the network. We would then create word and sentence vectors from our hidden states. This creates 768-dimensional token embedding vectors, on which we can then apply principal component analysis to plot the results using 2 axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d1c70e77-63ff-44c4-badd-242127365f55",
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
