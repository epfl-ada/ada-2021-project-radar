{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-f3b61a84-dfd9-4c5a-8d0d-adc85d14773e",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Ada Final Project - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-0f001ec5-33aa-493f-a3b2-709e241074e5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27337,
    "execution_start": 1636627834187,
    "source_hash": "bf4cc646",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python Standard Libraries\n",
    "import re\n",
    "import csv\n",
    "import bz2\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Install using conda\n",
    "# conda install matplotlib pandas ipywidgets beautifulsoup4 nltk\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#NLP libraries\n",
    "#\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import syllables\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download([\n",
    "     \"names\",\n",
    "     \"stopwords\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    " ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-ed300d7f-8297-46e9-b61e-0d90bdea681c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31,
    "execution_start": 1636627861542,
    "source_hash": "21f8699f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_FILE = 'data/hillary-and-trump-quotes-2016.json.bz2'\n",
    "\n",
    "CEFR_HTML_IN = \"data/cefr_data.html\"\n",
    "\n",
    "CEFR_CLEAN_CSV_IN = \"data/cefr_data_clean.csv\"\n",
    "CEFR_CSV_OUT = \"data/cefr_data.csv\"\n",
    "\n",
    "JEKYLL_PLOTS_PATH = \"docs/_includes/plots/\"\n",
    "    \n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "RANDOM_SAMPLE_SIZE = 3_000\n",
    "\n",
    "SEED = 92813\n",
    "\n",
    "HC = \"Hillary Clinton\"\n",
    "DT = \"Donald Trump\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-6564f599-70b8-46f2-9ecd-7c80c136de4b",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-1cd88084-afed-4a47-aad6-7c9a269eda38",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Initial Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-89611f62-5f85-4923-92ae-4dac4ffb8fda",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "For reference we include the code we executed on Google Colab to extract all quotes by **Hillary Clinton** and **Donald Trump** during the **year 2016** from the Quotebank dataset. This was a one time operation, which is why we did it outside of this notebook. All other algorithms we apply to the data will be/have been possibly iterated on for improvement, which is where the notebook format comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-6a758305-f895-4a95-bf2f-0d9056224eca",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```python\n",
    "PATH_TO_FILE = '/content/drive/MyDrive/Quotebank/quotes-2016.json.bz2'\n",
    "PATH_TO_OUT = '/content/drive/MyDrive/hillary-and-trump-quotes-2016.json.bz2'\n",
    "\n",
    "SPEAKER_NAMES = ['Hillary Clinton', 'Donald Trump']\n",
    "\n",
    "hits = 0\n",
    "\n",
    "with bz2.open(PATH_TO_FILE, 'rb') as s_file:\n",
    "    with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "\n",
    "        for instance in s_file:\n",
    "\n",
    "            instance = json.loads(instance)\n",
    "            speaker = instance['speaker']\n",
    "\n",
    "            if any(map(speaker.__contains__, desired_speakers)):\n",
    "\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "                hits += 1\n",
    "\n",
    "\n",
    "print(f\"Hits: {hits}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-d37921f2-e811-41fb-b687-d6cce16afa48",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29721,
    "execution_start": 1636627107359,
    "source_hash": "c87b5835",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_json(PATH_TO_FILE, lines=True, compression='bz2') #chunksize=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Produces a sample from the entire dataset as well one for each candidate\n",
    "#\n",
    "def get_df_samples(df, sample_size=RANDOM_SAMPLE_SIZE):\n",
    "\n",
    "    sample = df.sample(n=sample_size, random_state=SEED)\n",
    "\n",
    "    dt_sample = df[df['speaker'] == 'Donald Trump'].sample(n=sample_size, random_state=SEED)\n",
    "    hc_sample = df[df['speaker'] == 'Hillary Clinton'].sample(n=sample_size, random_state=SEED)\n",
    "    \n",
    "    return sample, dt_sample, hc_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-0ed8fdb1-7821-47ee-8779-13b0e9ab7998",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 2. Enhance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-6f097d4a-86b9-4128-9c58-ede70e65f96b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 298,
    "execution_start": 1636627137124,
    "source_hash": "57723a31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2 = df_1.copy()\n",
    "#\n",
    "# Extract first (highest) proba score, which is the one assigned to the \"speaker\" and place it in the proba column.\n",
    "#\n",
    "df_2['proba'] = df_2['probas'].apply(lambda probas : float(probas[0][1]))\n",
    "\n",
    "#\n",
    "# Only keep the date and ignore the time\n",
    "#\n",
    "df_2['date'] = pd.to_datetime(df_2['date'])\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-823ee243-60d7-489d-bdda-fe5b9e7e07f8",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Sample of enhanced, but dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-d933f3bb-8131-4599-af28-fc4cb32ea49c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 188,
    "execution_start": 1636627137429,
    "source_hash": "da860dd0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample, dt_sample, hc_sample = get_df_samples(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-c0f1883f-0af7-450b-a573-35ca66048a3f",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## 3. EDA and  Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-29c36457-8c23-4758-b6d6-36c47a66ead9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "We are only interested in quotes by **Hillary Clinton** and **Donald Trump** during the **year 2016**, specifically from 01/01/2016 - 01/01/2017. The subset loaded only contains the Quotebank quotes which have one (or both) of them as a possible speaker in the _speaker_ columns list and that lie in the specified time frame.\n",
    "\n",
    "Since the dataset was obtained using a ML model to extract and assign the quotes there will most likely be quotes which are faulty and quotes which have been assigned to the wrong speaker. The goal of the data cleaning is to remove such data points so that we can focus on working with as good data as possible.\n",
    "\n",
    "We must specify what makes a quote faulty and motivate this so that we remove as many bad quotes as possible while not removing any or as little actually correct quotes as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_patch = mpatches.Patch(color='blue', alpha=0.4, label='Hillary Clinton')\n",
    "red_patch = mpatches.Patch(color='red', alpha=0.4, label='Donald Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Histogram of all quotes grouped by Trump and CLinton for each day in 2016\n",
    "#\n",
    "def quotes_hist_split(df, weighted=False, use_log=False, n_bins=366): # One per day (366 days in 2016)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "\n",
    "    fig.suptitle(\"Histogram of {} Quotes by H.C. and D.T during 2016\".format('all' if weighted else 'unique'))\n",
    "\n",
    "    df_hc = df[df['speaker'] == 'Hillary Clinton']\n",
    "    df_dt = df[df['speaker'] == 'Donald Trump']\n",
    "\n",
    "    ax.hist(\n",
    "        [df_hc['date'], df_dt['date']],\n",
    "        weights=([df_hc['numOccurrences'] ,df_dt['numOccurrences']] if weighted else None),\n",
    "        bins=n_bins,\n",
    "        color=[\"blue\", \"red\"],\n",
    "        alpha=0.4,\n",
    "        log=use_log\n",
    "    )\n",
    "    \n",
    "    ax.set_ylabel(\"Frequency\" + (' (log)' if use_log else ''))\n",
    "    ax.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "quotes_hist_split(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram of all (Quotebank) Quotes in 2016](assets/histogram-quotebank-quotes-2016.png)\n",
    "\n",
    "After seing that there are suspicious periods of no or barely any quotes in the graph above we went back to the unfiltered (whole) dataset and plotted the distribution of all quote dates. The result can be seen in the graph above (computed in Google Colab) and suggests that this pattern is also present in the entire dataset, suggesting that this at least should not stem from a faulty filtration. \n",
    "\n",
    "**Note: The sharp dips are due to data outages on behalf of Spinn3r (From: Quotebank - A Corpus of Quotations from a Decade of News.pdf)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Histogram of all occurances per quote grouped by Trump and CLinton for each day in 2016\n",
    "#\n",
    "def occ_hist_split(df, n_bins=100, use_log=True):\n",
    "    \n",
    "    fig, ax =  plt.subplots(figsize=(10,5))\n",
    "\n",
    "    fig.suptitle(\"Histogram of the number of occurances of single quotes by H.C. and D.T during 2016\", fontsize=14)\n",
    "\n",
    "    df_hc = df[df['speaker'] == 'Hillary Clinton']\n",
    "    df_dt = df[df['speaker'] == 'Donald Trump']\n",
    "    ax.hist([df_hc['numOccurrences'], df_dt['numOccurrences']], log=use_log, bins=n_bins, color=[\"blue\", \"red\"], alpha=0.4)\n",
    "\n",
    "    ax.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "    plt.show();\n",
    "    plt.clf();\n",
    "\n",
    "df_test = df_3.sort_values('numOccurrences', ascending=False)\n",
    "\n",
    "display(df_test.head())\n",
    "print()\n",
    "\n",
    "#\n",
    "#print(df_3[df_3['numOccurrences'] < 300].loc[85071])\n",
    "#\n",
    "\n",
    "occ_hist_split(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplictates in regards to the quote content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[df_3.duplicated(subset=['quotation'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate quote, hence all quotations which have been used by multiple sources are indicated by the `numOccurances` column and further detailed by the list in the `urls` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proba assigned to Quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-65014d71-22c6-4993-9fb8-fc234790335a",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "#### Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-c5ae7b51-3b86-4cfd-8da2-dcd71ebdcbec",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Motivation**\n",
    "\n",
    "Certain quotes that the model assigned to Trump and Clinton have very low probabilities to actually be quotes by them inside of the text as by the computation of the model. We want to learn about the distribution of the probability of the assigned quotes so that we can take a decision on if and when to filter out certain quotes due to a too low probability computed for them by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-02a52593-947c-4632-8e03-d03c2ce35993",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Distribution of the Proba**\n",
    "\n",
    "Here we plot the distribution of a sample of 1,000 Trump and 1,000 Clinton quotes respectively. The reason we sample seperatley is given the fact that we want to ensure a large enough sample pool for both candidates, which is needed since there are more quotes assigned to Trump than Clinton. We also plot the two seperatly to make sure that we do not miss any differences in the dsitribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-b6d4516b-815d-4609-9515-358ea7df0e1d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1291,
    "execution_start": 1636627137617,
    "source_hash": "627ca6e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probas_hist_split(df, on_sample=True, n_bins=20):\n",
    "    \n",
    "    if on_sample:\n",
    "        _, dt_df, hc_df = get_df_samples(df)\n",
    "    else:\n",
    "        dt_df = df[df['speaker'] == 'Donald Trump']\n",
    "        hc_df = df[df['speaker'] == 'Hillary Clinton']\n",
    "\n",
    "    proba_bins = [round((1 / n_bins), 2) * i for i in range(0, n_bins  + 1)]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "    fig.suptitle(\"Distribution of the probability computed by the model to the assigned speaker\", fontsize=14)\n",
    "\n",
    "    axs[0, 0].set(xlim=(0, 1))\n",
    "    axs[0, 1].set(xlim=(0, 1))\n",
    "\n",
    "    axs[0, 0].hist(hc_df['proba'], bins=proba_bins, color='blue', alpha=0.4)\n",
    "    axs[0, 1].hist(dt_df['proba'], bins=proba_bins, color='red', alpha=0.4)\n",
    "\n",
    "    axs[1, 0].set(xlim=(0, 1))\n",
    "    axs[1, 1].set(xlim=(0, 1))\n",
    "\n",
    "    axs[1, 0].boxplot(hc_df['proba'], vert=False)\n",
    "    axs[1, 1].boxplot(dt_df['proba'], vert=False)\n",
    "\n",
    "\n",
    "    axs[0, 0].set_ylabel('Frequency')\n",
    "    axs[0, 0].title.set_text('Hillary Clinton (N={:,})'.format(len(hc_df)))\n",
    "\n",
    "    axs[0, 1].title.set_text('Donald Trump (N={:,})'.format(len(dt_df)))\n",
    "\n",
    "    axs[1, 1].set_xlabel('Computed Probability')\n",
    "    axs[1, 0].set_xlabel('Computed Probability')\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "#    \n",
    "# Run on a random sample of 1,0000 quotes per speaker (for speed) \n",
    "#\n",
    "example = df_3.copy()\n",
    "probas_hist_split(example, on_sample=True)\n",
    "\n",
    "    \n",
    "print(\"Hillary Clinton Summary Statistics\")\n",
    "display(hc_sample['proba'].describe())\n",
    "print()\n",
    "print(\"Donald Trump Summary Statistics\")\n",
    "display(hc_sample['proba'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-daffbef2-bf68-4186-9edb-eb7e8219fac2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From plotting the distribution of probabilities which the model computed to the quotes it assigned to Trump and Hillary respectively we could now act in at least 3 ways:\n",
    "\n",
    "1. Remove any quote which is below the min probability minus a small margin because they are outliers. **I.e. Only Remove outliers.**\n",
    "\n",
    "2. Set the cutoff even higher because we decide to consider quotes with, ex. less than 0.4 probability assigned to the candidate being the speaker too weak to consider it in further analysis.\n",
    "\n",
    "3. A further possibility could be to remove quotes, where the next best speaker assigned has a similair/close probability compare to the number one.\n",
    "   \n",
    "   Example: `[ [ 'Trump', 0.41 ], [ 'Kanye West', 0.39 ], ...]`\n",
    "\n",
    "**TODO for later:** What should we do? What is scientifically sound? How do we motivate it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering out datapoints with too low probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to remove quote with proba below a threshold.\n",
    "#\n",
    "def remove_low_proba_quotes(df, threshold):\n",
    "    return df[df['proba'] >= threshold].copy()\n",
    "\n",
    "example = df_3.copy()\n",
    "\n",
    "example_threshold = 0.7\n",
    "\n",
    "example = remove_low_proba_quotes(df_3, threshold=example_threshold)\n",
    "\n",
    "n_removed_lines = len(df_3) - len(example)\n",
    "percentage_removed = (n_removed_lines / len(df_3)) * 100\n",
    "\n",
    "print(\"Result of filter with threshold: {:}\\n\".format(example_threshold))\n",
    "print(\"Removed {:,.0f} datapoints or {:,.2f}% of the original dataset.\\n\".format(n_removed_lines, percentage_removed))\n",
    "\n",
    "probas_hist_split(example, on_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the speakers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_speaker_breakdown(df):\n",
    "    display(df.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation'])\n",
    "\n",
    "#\n",
    "# Check out the unique speakers in our dataset\n",
    "#\n",
    "display_speaker_breakdown(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our intial, rough, extraction algorithm extracted quotes by Trump's son and also some where Trump is titled \"President\". We assign will assign the \"president\" tittled quotes to Trump and filter out the  ones about his son."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-assignment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to assign quotes, where Trump is titlte \"President\" to his speaker name.\n",
    "#\n",
    "def assign_quotes_for_president_dt_to_dt(df):\n",
    "    \n",
    "    df = df.replace(\n",
    "        to_replace=['president Donald Trump', 'PRESIDENT Donald Trump', 'President Donald Trump'],\n",
    "        value='Donald Trump'\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "example = df_3.copy()\n",
    "example = assign_quotes_for_president_dt_to_dt(df_3)\n",
    "\n",
    "n_removed_lines = len(df_3) - len(example)\n",
    "percentage_removed = (n_removed_lines / len(df_3)) * 100\n",
    "\n",
    "print(\"Result of filter:\\n\")\n",
    "print(\"Removed {:,.0f} datapoints or {:,.2f}% of the original dataset.\\n\".format(n_removed_lines, percentage_removed))\n",
    "\n",
    "\n",
    "display_speaker_breakdown(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Removes quotes by different speakers\n",
    "#\n",
    "def only_keep_dt_and_hc_quotes(df):\n",
    "    return df[df['speaker'].isin(['Hillary Clinton', 'Donald Trump'])]\n",
    "\n",
    "example = only_keep_dt_and_hc_quotes(example)\n",
    "\n",
    "print(\"Result of filter: \\n\")\n",
    "display_speaker_breakdown(example)\n",
    "\n",
    "n_removed_lines = len(df_3) - len(example)\n",
    "percentage_removed = (n_removed_lines / len(df_3)) * 100\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Removed {:,.0f} datapoints or {:,.2f}% of the original dataset\".format(n_removed_lines, percentage_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-ae383ba4-e1d4-412e-a09b-1eb2e31ebf62",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### \"Nonsense\" content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a288c881-2a25-42c7-9a10-2df0459cbe9c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**TODO**: We should ideally check for the quality of the quotes. There are certainly some faulty quotes and maybe even gibberish in the dataset but it is hopefully very limited in scope. We should nonetheless attempt to look for faulty/gibbersih quotes which were extracted by the model and remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"data-gap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Quotes per day of entire 2016 Quotebbank\n",
    "df_original_hist = pd.read_csv(\"data/unique-quotes-per-day-2016.csv\")[['date', 'n_unique_quotes']]\n",
    "df_original_hist['date'] = pd.to_datetime(df_original_hist['date'])\n",
    "\n",
    "fig, (left_ax, right_ax) = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "fig.suptitle(\"Distribution of Unique Quotes in Quotebank for 2016\", fontsize=16)\n",
    "\n",
    "left_ax.set_title(\"Trump and Clinton only\")\n",
    "left_ax.hist(df_3['date'], bins=366, color='purple', alpha=0.4)\n",
    "\n",
    "right_ax.set_title(\"All Quotes\")\n",
    "right_ax.hist(df_original_hist['date'], weights=df_original_hist['n_unique_quotes'], bins=366, color='gray', alpha=0.4)\n",
    "\n",
    "xticks = ['2016-03-15', '2016-06-15', '2016-09-15', '2016-12-15']\n",
    "xticks_labels = ['15 March', '15 June', '15 September', '15 December']\n",
    "\n",
    "left_ax.set_xticks(xticks)\n",
    "left_ax.set_xticklabels(xticks_labels)\n",
    "\n",
    "right_ax.set_xticks(xticks)\n",
    "right_ax.set_xticklabels(xticks_labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patches for the Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15,8))\n",
    "\n",
    "fig.suptitle(\"Identifying the 'Cutoff' Dates\", fontsize=16)\n",
    "\n",
    "#\n",
    "# First Gap\n",
    "#\n",
    "axs[0, 0].set_title(\"Gap: February - March\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-02-01') & (df_original_hist['date'] <= '2016-03-01')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[0, 0].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-02-01', '2016-02-15', '2016-03-01']\n",
    "xticks_labels = ['01 Feb', '15 Feb', '01 March']\n",
    "\n",
    "axs[0, 0].set_xticks(xticks)\n",
    "axs[0, 0].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_left = '2016-02-12'\n",
    "v_l = mlines.Line2D([cutoff_left, cutoff_left], [0,50_000], color='red')\n",
    "axs[0, 0].add_line(v_l)\n",
    "axs[0, 0].text(bins[6]-0.5, 50_000,  cutoff_left, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "cutoff_right = '2016-02-25'\n",
    "v_l = mlines.Line2D([cutoff_right, cutoff_right], [0,50_000], color='red')\n",
    "axs[0, 0].add_line(v_l)\n",
    "axs[0, 0].text(bins[18]-0.5, 50_000,  cutoff_right, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "#\n",
    "# Second Gap\n",
    "#\n",
    "axs[0, 1].set_title(\"Gap: April\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-04-07') & (df_original_hist['date'] <= '2016-05-01')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[0, 1].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-04-07', '2016-04-15', '2016-05-01']\n",
    "xticks_labels = ['07 April', '15 April', '01 May']\n",
    "\n",
    "axs[0, 1].set_xticks(xticks)\n",
    "axs[0, 1].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_left = '2016-04-17'\n",
    "v_l = mlines.Line2D([cutoff_left, cutoff_left], [0,50_000], color='red')\n",
    "axs[0, 1].add_line(v_l)\n",
    "axs[0, 1].text(bins[3], 50_000,  cutoff_left, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "#\n",
    "# Third Gap\n",
    "#\n",
    "axs[0, 2].set_title(\"Gap: May - June\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-05-15') & (df_original_hist['date'] <= '2016-06-15')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[0, 2].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-05-15', '2016-06-15']\n",
    "xticks_labels = ['15 May', '15 June']\n",
    "\n",
    "axs[0, 2].set_xticks(xticks)\n",
    "axs[0, 2].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_right = '2016-06-01'\n",
    "v_l = mlines.Line2D([cutoff_right, cutoff_right], [0,50_000], color='red')\n",
    "axs[0, 2].add_line(v_l)\n",
    "axs[0, 2].text(bins[19], 50_000,  cutoff_right, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "#\n",
    "# Fourth Gap\n",
    "#\n",
    "axs[1, 0].set_title(\"Gap: June - July\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-06-15') & (df_original_hist['date'] <= '2016-07-15')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[1, 0].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-06-15', '2016-07-01', '2016-07-15']\n",
    "xticks_labels = ['15 June', '01 July', '15 July']\n",
    "\n",
    "axs[1, 0].set_xticks(xticks)\n",
    "axs[1, 0].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_right = '2016-06-30'\n",
    "v_l = mlines.Line2D([cutoff_right, cutoff_right], [0,50_000], color='red')\n",
    "axs[1, 0].add_line(v_l)\n",
    "axs[1, 0].text(bins[6], 50_000,  cutoff_right, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "#\n",
    "# Fifth Gap\n",
    "#\n",
    "axs[1, 1].set_title(\"Gap: September - October\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-09-15') & (df_original_hist['date'] <= '2016-10-15')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[1, 1].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-09-15', '2016-10-01', '2016-10-15']\n",
    "xticks_labels = ['15 September', '01 October', '15 October']\n",
    "\n",
    "axs[1, 1].set_xticks(xticks)\n",
    "axs[1, 1].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_right = '2016-10-01'\n",
    "v_l = mlines.Line2D([cutoff_right, cutoff_right], [0,50_000], color='red')\n",
    "axs[1, 1].add_line(v_l)\n",
    "axs[1, 1].text(bins[18], 50_000,  cutoff_right, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "#\n",
    "# Sixt Gap\n",
    "#\n",
    "axs[1, 2].set_title(\"Gap: November - December\")\n",
    "\n",
    "gap = df_original_hist[(df_original_hist['date'] >= '2016-11-15') & (df_original_hist['date'] <= '2016-12-15')]\n",
    "gap_days = len(gap)\n",
    "\n",
    "_, bins, patches = axs[1, 2].hist(gap['date'], weights=gap['n_unique_quotes'], bins=gap_days, color='gray', alpha=0.4, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "xticks = ['2016-11-15', '2016-12-01', '2016-12-15']\n",
    "xticks_labels = ['15 November', '01 December', '15 December']\n",
    "\n",
    "axs[1, 2].set_xticks(xticks)\n",
    "axs[1, 2].set_xticklabels(xticks_labels)\n",
    "\n",
    "cutoff_right = '2016-11-30'\n",
    "v_l = mlines.Line2D([cutoff_right, cutoff_right], [0,50_000], color='red')\n",
    "axs[1, 2].add_line(v_l)\n",
    "axs[1, 2].text(bins[6], 50_000,  cutoff_right, style='italic',\n",
    "        bbox={'facecolor': 'red', 'alpha': 0.8, 'pad': 10})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Removes any quotes which are inside the data gaps as found above.\n",
    "#\n",
    "DATE_CUTOFFS = [\n",
    "    ('2016-02-12', '2016-02-25'),\n",
    "    ('2016-04-17', '2016-06-01'),\n",
    "    ('2016-06-30', '2016-10-01'),\n",
    "    ('2016-11-30', '2017-01-01')\n",
    "]\n",
    "\n",
    "def remove_quotes_inside_data_gaps(df):\n",
    "    df = df.copy()\n",
    "    df = df[\n",
    "        ( (DATE_CUTOFFS[0][0] <  df['date']) & (df['date'] < DATE_CUTOFFS[0][1]) ) |\n",
    "        ( (DATE_CUTOFFS[1][0] <  df['date']) & (df['date'] < DATE_CUTOFFS[1][1]) ) |\n",
    "        ( (DATE_CUTOFFS[2][0] <  df['date']) & (df['date'] < DATE_CUTOFFS[2][1]) ) |\n",
    "        ( (DATE_CUTOFFS[3][0] <  df['date']) & (df['date'] < DATE_CUTOFFS[3][1]) )\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Pre Filter\")\n",
    "quotes_hist_split(df_3, weighted=False, use_log=True)\n",
    "print(\"Post Filter\")\n",
    "example = remove_quotes_inside_data_gaps(df_3)\n",
    "quotes_hist_split(example, weighted=False, use_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "Using our insights from the EDA we apply the different filters to \"clean\" our dataset, which is then ready for proper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_2.copy()\n",
    "print(\"Pre-Cleaning:  {:>10,} Quotes\".format(len(df_cleaned)))\n",
    "\n",
    "# Reassign quotes to Trump\n",
    "df_cleaned = assign_quotes_for_president_dt_to_dt(df_cleaned)\n",
    "\n",
    "# Remove quotes which are of  other speakers\n",
    "df_cleaned = only_keep_dt_and_hc_quotes(df_cleaned)\n",
    "\n",
    "# Remove any quote inside  our identified data gaps.\n",
    "df_cleaned = remove_quotes_inside_data_gaps(df_cleaned)\n",
    "\n",
    "n_removed = len(df_2) - len(df_cleaned)\n",
    "per_removed = n_removed / len(df_2)\n",
    "print(\"Removed:       {:>10,} Quotes or {:.2%} of the Original Data\".format(n_removed, per_removed))\n",
    "print(\"Post-Cleaning: {:>10,} Quotes\".format(len(df_cleaned)))\n",
    "\n",
    "display(df_cleaned.head())\n",
    "quotes_hist_split(df_cleaned, weighted=False, use_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This should  be removed and is just so the code doesn't break after (Dean) having resturctured the cleaning process.\n",
    "df_3_2 = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sample, dt_cleaned_sample, hc_cleaned_sample = get_df_samples(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Our Focus Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Introduction & Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What data  do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hc_quotes = sum(df_cleaned[df_cleaned['speaker'] == 'Hillary Clinton']['numOccurrences'])\n",
    "n_dt_quotes = sum(df_cleaned[df_cleaned['speaker'] == 'Donald Trump']['numOccurrences'])\n",
    "\n",
    "labels = 'Hillary', 'Trump'\n",
    "sizes = [n_hc_quotes, n_dt_quotes]\n",
    "explode = (0, 0.10)\n",
    "\n",
    "fig, (left_ax, right_ax) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "fig.suptitle(\"Breakdown of Quotes\",  fontsize=16)\n",
    "\n",
    "#\n",
    "# Amounted Quoted\n",
    "#\n",
    "left_ax.set_title(\"Times Quoted\")\n",
    "\n",
    "n_hc_quotes = sum(df_cleaned[df_cleaned['speaker'] == 'Hillary Clinton']['numOccurrences'])\n",
    "n_dt_quotes = sum(df_cleaned[df_cleaned['speaker'] == 'Donald Trump']['numOccurrences'])\n",
    "total_times_quoted = sum(df_cleaned['numOccurrences'])\n",
    "\n",
    "left_ax.pie(\n",
    "    [n_hc_quotes, n_dt_quotes],\n",
    "    labels=labels,\n",
    "    #autopct='%1.1f%%',\n",
    "    autopct=lambda p: '{:,}'.format(round(p * total_times_quoted / 100)),\n",
    "    colors=['blue', 'red'],\n",
    "    startangle=90,\n",
    "    textprops={'size': 'larger'},\n",
    "    wedgeprops = {\"alpha\": 0.4})\n",
    "\n",
    "#\n",
    "# Unique  Quotes\n",
    "#\n",
    "right_ax.set_title(\"Unique Quotes\")\n",
    "\n",
    "n_unique_hc_quotes = len(df_cleaned[df_cleaned['speaker'] == 'Hillary Clinton'])\n",
    "n_unique_dt_quotes = len(df_cleaned[df_cleaned['speaker'] == 'Donald Trump'])\n",
    "\n",
    "right_ax.pie(\n",
    "    [n_unique_hc_quotes, n_unique_dt_quotes],\n",
    "    labels=labels,\n",
    "    #autopct='%1.1f%%',\n",
    "    autopct=lambda p: '{:,}'.format(round(p * len(df_cleaned) / 100)),\n",
    "    colors=['blue', 'red'],\n",
    "    startangle=90,\n",
    "    textprops={'size': 'larger'},\n",
    "    wedgeprops = {\"alpha\": 0.4})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Media Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do media outlets portray Trump and Clinton differently? Do media outlets quote the two candidates equally much? Does the bias of the news outlet correlate with the quotes they report?\n",
    "\n",
    "Let's take two of the biggest outlets with political leaning views, CNN and Breitbart, and compare the distribution of Trump quotes with Clinton quotes, and also see how positive or negative they are.\n",
    "\n",
    "The NLTK library will also be used for sentiment analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scripts.allsides import strip_url\n",
    "\n",
    "def flatten(_list):\n",
    "    return list(itertools.chain(*_list))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def valence(quote):\n",
    "    return sia.polarity_scores(quote)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_urls = flatten(cleaned_sample[\"urls\"].to_list())\n",
    "stripped_urls = list({strip_url(url) for url in full_urls})\n",
    "pd.DataFrame(pd.Series(stripped_urls).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_by_outlet = dict()\n",
    "\n",
    "for i, row in cleaned_sample.iterrows():\n",
    "    quote, speaker, urls = row[\"quotation\"], row[\"speaker\"], row[\"urls\"]\n",
    "    \n",
    "    for url in urls:\n",
    "        outlet = strip_url(url)\n",
    "        \n",
    "        v = quotes_by_outlet.get(outlet)\n",
    "        if v is None:\n",
    "            v = {\"Hillary Clinton\": [], \"Donald Trump\": []}\n",
    "            \n",
    "        # A quote cannot appear more than once per outlet.\n",
    "        if quote not in v[speaker]:\n",
    "            v[speaker].append(quote)\n",
    "        quotes_by_outlet[outlet] = v\n",
    "\n",
    "df_by_outlet = pd.DataFrame.from_dict(quotes_by_outlet, orient=\"index\")\n",
    "df_by_outlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the media bias dataset. The bias is translated from survey answers (\"left-center\", \"center\", \"right\"...)\n",
    "and the weight is based on the confidence in the bias rating (a non-linear function combining the number of votes and the agreement rate).\n",
    "You can find the cleaning and transformation process in `scripts/allsides.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mb = pd.read_csv(\"data/allsides.csv\")\n",
    "df_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mb.groupby(\"bias\").count()[\"total_votes\"].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mb_valence = df_by_outlet.merge(df_mb, left_index=True, right_on=\"url\")\n",
    "df_mb_valence.index = df_mb_valence[\"name\"]\n",
    "df_mb_valence.drop(\"name\", axis=1, inplace=True)\n",
    "\n",
    "map_quotes = lambda quotes: [valence(q) for q in quotes]\n",
    "df_mb_valence[\"hc_valence\"] = df_mb_valence[HC].apply(map_quotes)\n",
    "df_mb_valence[\"dt_valence\"] = df_mb_valence[DT].apply(map_quotes)\n",
    "            \n",
    "df_mb_valence.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mb_valence[df_mb_valence.bias == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hc = df_mb_valence.copy()\n",
    "df_hc[\"valence\"] = df_hc[\"hc_valence\"]\n",
    "df_hc = df_hc.drop([HC, DT, \"hc_valence\", \"dt_valence\"], axis=1)\n",
    "df_hc = df_hc.explode(\"valence\")\n",
    "df_hc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt = df_mb_valence.copy()\n",
    "df_dt[\"valence\"] = df_dt[\"dt_valence\"]\n",
    "df_dt = df_dt.drop([HC, DT, \"hc_valence\", \"dt_valence\"], axis=1)\n",
    "df_dt = df_dt.explode(\"valence\")\n",
    "df_dt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist2d(df, candidate):\n",
    "    \n",
    "    if candidate == DT:\n",
    "        color = px.colors.qualitative.Plotly[1]\n",
    "        x = 1\n",
    "    elif candidate == HC:\n",
    "        color = px.colors.qualitative.Plotly[0]\n",
    "        x = 0.4\n",
    "    else:\n",
    "        raise(f\"Expected candidate to be one of [{DT}, {HC}]\")\n",
    "              \n",
    "    args = {\n",
    "        \"x\": df[\"bias\"],\n",
    "        \"y\": df[\"valence\"],\n",
    "        \"z\": df[\"weight\"],\n",
    "        \"histfunc\": \"sum\",\n",
    "        \"histnorm\": \"probability\",\n",
    "        \"colorbar\": dict(\n",
    "            len=1.05,\n",
    "            x=x,\n",
    "            y=0.49,\n",
    "            dtick=0.04\n",
    "        ),\n",
    "        \"colorscale\": [\"white\", color],\n",
    "        \"showscale\": True,\n",
    "    }\n",
    "    return go.Histogram2d(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    cols=2,\n",
    "    shared_yaxes=\"columns\",\n",
    "    x_title='Media bias',\n",
    "    y_title='Sentiment',\n",
    "    horizontal_spacing=0.2,\n",
    "    subplot_titles=[HC, DT])\n",
    "           \n",
    "# customize font and legend orientation & position\n",
    "fig.update_layout(\n",
    "    font_family=\"Rockwell\",\n",
    "    legend=dict(\n",
    "        title=None,\n",
    "        orientation=\"h\",\n",
    "        y=1,\n",
    "        yanchor=\"bottom\",\n",
    "        x=0.5,\n",
    "        xanchor=\"center\",\n",
    "    ),\n",
    "    title=\"Distribution of sentiment and media bias\",\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [-2, -1, 0, 1, 2],\n",
    "    ticktext = [\"Left\", \"LC\", \"Center\", \"RC\", \"Right\"],    \n",
    "    tickangle = 0,\n",
    ")\n",
    "\n",
    "fig.add_trace(plot_hist2d(df_hc, HC), row=1, col=1)\n",
    "fig.add_trace(plot_hist2d(df_dt, DT), row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(fig, outlet_name):\n",
    "    \n",
    "    if outlet_name in df_mb_valence.index:\n",
    "        outlet = df_mb_valence.loc[outlet_name]\n",
    "        bias = outlet.bias\n",
    "    else:\n",
    "        raise(ValueError(f\"{outlet_name} is not a valid outlet name\"))\n",
    "    \n",
    "    nbins = 5\n",
    "    \n",
    "    args = dict(\n",
    "        histnorm= \"probability\",\n",
    "        showlegend=bool(bias==-2),\n",
    "        ybins=dict(\n",
    "            start=-1.1,\n",
    "            end=1.1,\n",
    "            size=0.2,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    args_hc = dict(\n",
    "        **args,\n",
    "        name=HC,\n",
    "        marker=dict(color=px.colors.qualitative.Plotly[0]),\n",
    "        y=outlet[\"hc_valence\"],\n",
    "    )\n",
    "    \n",
    "    args_dt = dict(\n",
    "        **args,\n",
    "        name=DT,\n",
    "        marker=dict(color=px.colors.qualitative.Plotly[1]),\n",
    "        y=outlet[\"dt_valence\"],\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Histogram(args_hc), col=bias+3, row=1)\n",
    "    fig.add_trace(go.Histogram(args_dt), col=bias+3, row=1)\n",
    "    \n",
    "    title_text=f\"{outlet_name.split(' (')[0]}\"\n",
    "    fig.update_xaxes(title_text=title_text, col=bias+3, row=1)\n",
    "\n",
    "\n",
    "outlet_names = [\"CNN (Online News)\", \"Washington Post\", \"The Hill\", \"Newsmax (News)\", \"Breitbart News\"]\n",
    "    \n",
    "fig = make_subplots(\n",
    "    cols=5,\n",
    "    shared_yaxes=True,\n",
    "    x_title='Probability of occurrence',\n",
    "    y_title='Sentiment',\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [0.2, 0.4],\n",
    "    ticktext = [\"\", \"\"],    \n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    tickmode = 'array',\n",
    "    tickvals = [-1, -0.5, 0, 0.5, 1],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    font_family=\"Rockwell\",\n",
    "    legend=dict(\n",
    "        title=\"Speaker\",\n",
    "        orientation=\"h\",\n",
    "        y=1,\n",
    "        yanchor=\"bottom\",\n",
    "        x=0.5,\n",
    "        xanchor=\"center\",\n",
    "    ),\n",
    "    title=\"Distribution of sentiment for certain media outlets\",\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "for name in outlet_names:\n",
    "    plot_hist(fig, name)\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Political Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to track the different political topics that the candidates focused on according to the content of their quotes. From this we aim to learn the importance of the topics in a absolut relative context but also in regards to when a certain topic might have been very present and then. disappeared for some time. We furthermore want to see if there might be a candidate which sparked a topic or at least started talking/being quoted about it first.\n",
    "\n",
    "Bewlow we show an example of a topic in regards to Obamacare, which we all remember to be a important topic during the debate.\n",
    "\n",
    "#### Example: Obama Care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-07952c10-e99f-4cb2-8b2f-a25e0bf8546d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3516,
    "execution_start": 1636627350223,
    "source_hash": "fa48ee91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "OBAMACARE_REGEX = '(obama care)|obamacare' # Not perfect, proof of concept...\n",
    "\n",
    "df_obamacare = df_3_2[df_3_2['quotation'].str.contains(pat = OBAMACARE_REGEX, regex = True, flags=re.IGNORECASE)].sort_values('date')\n",
    "df_obamacare['date'] = df_obamacare['date'].apply(lambda date : date.date())\n",
    "\n",
    "df_obamacare_hc = df_obamacare[df_obamacare['speaker'] == 'Hillary Clinton']\n",
    "df_obamacare_dt = df_obamacare[df_obamacare['speaker'] == 'Donald Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-b295fa30-4d14-41c6-b300-892aaf1f6de3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4578,
    "execution_start": 1636627353740,
    "source_hash": "40817699",
    "tags": []
   },
   "outputs": [],
   "source": [
    "quotes_hist_split(df_obamacare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_topics_histogram(df_hc, df_dt, bins=52, topic_name=\"\"):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    kwargs = {\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\": bins,\n",
    "    }\n",
    "\n",
    "    axs[0].hist(df_hc['date'], weights=df_hc['numOccurrences'], color=\"b\", **kwargs)\n",
    "    axs[1].hist(df_dt['date'], weights=df_dt['numOccurrences'], color=\"r\", **kwargs)\n",
    "\n",
    "    fig.suptitle(\"Mentions of the Topic {} in 2016 by candidate\".format(topic_name), fontsize=16)\n",
    "\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "\n",
    "    axs[0].title.set_text('Hillary Clinton')\n",
    "    axs[0].set_xlabel('Date')\n",
    "\n",
    "    axs[1].title.set_text('Donald Trump')\n",
    "    axs[1].set_xlabel('Date')\n",
    "    \n",
    "    ticks = [f\"2016-0{i * 3}-01\" for i in range(1,4)] + [\"2016-12-01\"]\n",
    "\n",
    "    axs[0].set_xticks(ticks)\n",
    "    axs[1].set_xticks(ticks)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "quote_topics_histogram(df_obamacare_hc, df_obamacare_dt, topic_name=\"Obamacare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obserrvation from Example\n",
    "\n",
    "Hillary Clinton talked less about Obamacare but instead more about Healthcare... To be explored further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# If set to True the entire topic extraction pipeline will be executed which takes a lot of time.\n",
    "# If set to False if is possible to execute parts of the pipeline (see sections below).\n",
    "#\n",
    "TOPICS_FROM_SCRATCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a quick look of the textual data we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We create one blob of text, consisting of all quotes in the dataset.\n",
    "#\n",
    "merged_quotes =  ' '.join(df_cleaned['quotation'].values)\n",
    "#\n",
    "print(\"Quotes in Dataset:          {:>10,}\".format(len(df_cleaned)))\n",
    "print(\"Characters in Text:         {:>10,}\".format(len(merged_quotes)))\n",
    "print(\"Spacy maximum:              {:>10,}\".format(1_000_000))\n",
    "print()\n",
    "print(\"Clinton Quotes in Dataset:  {:>10,}\".format(len(df_cleaned[df_cleaned['speaker'] == 'Hillary Clinton'])))\n",
    "print(\"Trump Quotes in Dataset:    {:>10,}\".format(len(df_cleaned[df_cleaned['speaker'] == 'Donald Trump'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the amount of data is too big to run on my computer we need to break it down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_quotes_aggregate = df_cleaned.groupby([df_cleaned['date'].dt.date]).apply(lambda group : group['quotation'].values).to_frame(name='quotes') #, columns=['date', 'quotes'])\n",
    "\n",
    "print(\"{:} days with Quotes.\".format(len(daily_quotes_aggregate)))\n",
    "display(daily_quotes_aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining our NLP Model\n",
    "\n",
    "We use this model to process the each quote in preparation for the topic extracion model (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # Load Spacy English\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # Remove stopwords, punctuation and words with less than 3 characters.\n",
    "    tokens = [token for token in doc if not token.is_stop]\n",
    "    tokens = [token for token in tokens if not token.is_punct]\n",
    "    doc = [token.text for token in tokens if len(token) > 2]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_PROCESS_QUOTES = False or TOPICS_FROM_SCRATCH # Should take < 15 minutes.\n",
    "\n",
    "NLP_MODEL_PATH = 'topics/nlp-model'\n",
    "DOCUMENTS_FILE = 'topics/documents.pickle'\n",
    "\n",
    "if NLP_PROCESS_QUOTES:\n",
    "\n",
    "    print(\"Processing quotes using Spacy NLP..\")\n",
    "\n",
    "    daily_quotes = daily_quotes_aggregate['quotes'].values\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for day_quotes in tqdm(daily_quotes):\n",
    "        merged_quotes = ''.join(day_quotes)\n",
    "\n",
    "        documents.append(nlp(merged_quotes))\n",
    "\n",
    "    # Write to disk\n",
    "    nlp.to_disk(NLP_MODEL_PATH)\n",
    "    docs_file = open(DOCUMENTS_FILE, 'wb')\n",
    "    pickle.dump(documents, docs_file)\n",
    "\n",
    "else:\n",
    "    print(\"Reading from file...\")\n",
    "    nlp = nlp.from_disk(NLP_MODEL_PATH)\n",
    "    \n",
    "    docs_file = open(DOCUMENTS_FILE, \"rb\")\n",
    "    documents = pickle.load(docs_file)\n",
    "\n",
    "print(\"\\nSample of 1 document:\\n\")\n",
    "print(documents[0][0:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Corpus and ID2words for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_dictionary(documents, max_freq=0.5, min_wordcount=5):\n",
    "    words = corpora.Dictionary(documents)\n",
    "    print(\"Words in Corpus:     {:>8,}     |  After Spacy NLP\".format(len(words)))\n",
    "\n",
    "    words.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "    print(\"Words in Corpus:     {:>8,}     |  After removing words with too low/high frequency)\\n\".format(len(words)))\n",
    "\n",
    "    return words\n",
    "\n",
    "def build_corpus(words, documents):\n",
    "\n",
    "    # Turns each document into a bag of words.\n",
    "    corpus = [words.doc2bow(document) for document in documents]\n",
    "    print(\"Documents in Corpus: {:>8}     |  1 document per day in our data range\\n\".format(len(corpus)))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = build_word_dictionary(documents)\n",
    "corpus = build_corpus(words, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train LDA modelto extract topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read modelfrom Disk or train from scratch.\n",
    "#\n",
    "NEW_MODEL = False or TOPICS_FROM_SCRATCH\n",
    "\n",
    "N_TOPICS = 50  # Number of topics to extract\n",
    "\n",
    "GENSIM_MODEL_FILE_NAME='topics/gensim-model/lda-model.gensim'\n",
    "\n",
    "if NEW_MODEL:\n",
    "    print(\"Training new model...\")\n",
    "    model = LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=words,\n",
    "        num_topics=N_TOPICS,\n",
    "        workers=6,\n",
    "        passes=50,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model.save(GENSIM_MODEL_FILE_NAME)\n",
    "    print(\"Done training new model. Saved to file...\")\n",
    "else:\n",
    "    print(\"Reading from file...\")\n",
    "    model = LdaMulticore.load(GENSIM_MODEL_FILE_NAME)\n",
    "\n",
    "EXAMPLE_TOPIC_ID = 0\n",
    "\n",
    "print(\"Example:\\n\")\n",
    "print(\"Displaying the function of  topic with ID: {:}\\n\".format(EXAMPLE_TOPIC_ID))\n",
    "model.print_topic(EXAMPLE_TOPIC_ID)\n",
    "print(\"\\nShow  top 3 words of some topics:\")\n",
    "display(model.show_topics(num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the extracted topics.\n",
    "\n",
    "#visualizable_data = pyLDAvis.gensim_models.prepare(model, corpus, words)\n",
    "#pyLDAvis.display(visualizable_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign each quote to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# If set to TRUE we will re-assign all quotes based on the latest model.\n",
    "#\n",
    "RE_ASSIGN = False or TOPICS_FROM_SCRATCH # takes alot of time (>15 minutes).\n",
    "\n",
    "#\n",
    "# Extracting the most likely topic out of a quote (or any text).\n",
    "#\n",
    "def assign_topic_id(quote):\n",
    "\n",
    "        document = nlp(quote)\n",
    "        document_bow = words.doc2bow(document)\n",
    "\n",
    "        topics = model.get_document_topics(document_bow)\n",
    "        topics = sorted(topics, key= lambda topic : topic[1], reverse=True)\n",
    "    \n",
    "        top_topic = topics[0][0]\n",
    "        \n",
    "        return top_topic\n",
    "\n",
    "#\n",
    "# Get dataframe with topic id assigned to each quote/row\n",
    "# (Recomputed or Read from File)\n",
    "#\n",
    "if RE_ASSIGN:\n",
    "    print(\"Re-assigning each quote/row based  on current model. (will take a while!)\")\n",
    "    df_cleaned['topic_id'] = df_cleaned['quotation'].apply(lambda quote : assign_topic_id(quote))\n",
    "\n",
    "    df_cleaned_with_topics  = df_cleaned.copy()\n",
    "\n",
    "    #  Write to disk\n",
    "    df_cleaned_with_topics.to_csv('data/df_cleaned_with_topics.csv')\n",
    "else:\n",
    "    print(\"Reading from file...\")\n",
    "    df_cleaned_with_topics = pd.read_csv('data/df_cleaned_with_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "fig.suptitle(\"Histogram of Quotes per Topic\", fontsize=16)\n",
    "\n",
    "hist = np.bincount(df_cleaned_with_topics['topic_id'])\n",
    "\n",
    "ax.bar(range(50), hist, color='green', alpha=0.4)\n",
    "ax.set_xlabel(\"Topic  Number\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-89407345-46f3-4822-af79-aa219e1c362b",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Q3: Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-872c0020-2014-496f-b174-2a714fe55dcb",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### a) Intellectuality of Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-ba5fd85d-d420-4cf7-aca7-c62741f0ee50",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Idea 1: Count the syllables in each candidate's consolidated quotes and compare distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-ff68b023-ba41-43aa-8e52-2ba0328600e9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1636627516845,
    "source_hash": "e1eb850e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def syllables_hist(dt_text: str, hc_text: str):\n",
    "\n",
    "    dt_syl = list(map(syllables.estimate, dt_text.split()))\n",
    "    hc_syl = list(map(syllables.estimate, hc_text.split()))\n",
    "\n",
    "    kwargs = {\n",
    "        \"x\"    : [hc_syl, dt_syl],\n",
    "        \"label\" : [\"Hillary Clinton\", \"Donald Trump\"],\n",
    "        \"color\" : [\"b\", \"r\"],\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\" : range(1, 6),\n",
    "        \"density\" : True,\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "\n",
    "    plt.hist(**kwargs)\n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram of number of syllables\", fontsize=16)\n",
    "    plt.xlabel(\"Number of syllables in a Word\")\n",
    "    plt.ylabel(\"Probability of Occuring\")\n",
    "\n",
    "    # TODO generalize this\n",
    "    plt.xticks(range(1, 5), range(1, 5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-53a7c04e-35a8-4276-a1ef-5c1d6fe205f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1636627518342,
    "source_hash": "9ad549ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc_text = ' '.join(hc_cleaned_sample[\"quotation\"])\n",
    "dt_text = ' '.join(dt_cleaned_sample[\"quotation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-8be22ba2-c920-46eb-aabd-da3c4954f2f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4005,
    "execution_start": 1636627520438,
    "source_hash": "8e9bff90",
    "tags": []
   },
   "outputs": [],
   "source": [
    "syllables_hist(dt_text, hc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-5efdf486-dff7-40c2-9c6c-0636b6a629cf",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Idea 2: Look at the CEFR language level. We can measure the language level for all words in each candidate's consolidated quotes and compare the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-dd63e800-673b-42f3-a2f9-57c9c8b8b230",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "But first we need to get data on words. We have the HTML for a web page containing these data, and we can put them in the form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-5a090841-8650-42e0-bce0-e08fe18a1ddb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636627750559,
    "source_hash": "bb24539e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bs_to_csv(soup, csv_writer):\n",
    "    table = soup.find(\"tbody\")\n",
    "    tr_rows = table.find_all(\"tr\")\n",
    "    csv_writer.writerow([\"word\", \"guideword\", \"level\", \"part of speech\", \"topic\"])\n",
    "    csv_writer.writerows(\n",
    "        [[t.text for t in r.find_all(\"td\")][:-1] for r in tr_rows]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-761bac2b-b461-43cf-8630-f6a3811e3088",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25741,
    "execution_start": 1636627762910,
    "source_hash": "a020fbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape the HTML file and write in CSV format to file.\n",
    "with open(CEFR_HTML_IN) as fp, open(CEFR_CSV_OUT, 'w') as csv_out:\n",
    "    soup = BeautifulSoup(fp)\n",
    "    csv_writer = csv.writer(csv_out, delimiter=',')\n",
    "    bs_to_csv(soup, csv_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-9a1ff974-a9b3-4bf4-a867-b1519bc3f6eb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 283,
    "execution_start": 1636627800203,
    "source_hash": "11315500",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr = pd.read_csv(CEFR_CSV_OUT)\n",
    "\n",
    "display(df_cefr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-4fd8864f-f336-4da5-bdc8-bc782b7f6a43",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The data includes idioms as well as nouns; this is intelligible for a human, but it might be a bit of trouble to make it work here, so we remove them.\n",
    "We'll only need the `word` and `level` columns, so we remove the others as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-250974a1-a51d-44df-b15e-36b1f85972b2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627822556,
    "source_hash": "7d9ff058",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr = df_cefr[[len(w.split()) == 1 for w in df_cefr[\"word\"]]]\\\n",
    "                 .filter(items=[\"word\", \"level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00042-0cd59360-8668-472a-ba85-e0099da5bb22",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we set everything to lowercase. Also it seems that some words have punctuation surrounding them, which is not desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-058ad49b-70ac-46aa-9492-04b0e59359d1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 165,
    "execution_start": 1636627861574,
    "source_hash": "4515b720",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr[\"word\"] = df_cefr[\"word\"].transform(\n",
    "    lambda w: w.lower()\\\n",
    "               .translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "display(df_cefr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-2ffff90e-01d2-4cda-94af-77e2af2591a8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Also, the same word can appear multiple times (as there might be different phrases in which the word means something slightly different) so we aggregate the levels using the median level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-72d203e6-dc7e-496e-8593-b9569fec7518",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636627870310,
    "source_hash": "316ad935",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_level_map = {\n",
    "    \"A1\": 1,\n",
    "    \"A2\": 2,\n",
    "    \"B1\": 3,\n",
    "    \"B2\": 4,\n",
    "    \"C1\": 5,\n",
    "    \"C2\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-36bcb5df-a186-429d-bf1d-18f6c35a39b6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627870310,
    "source_hash": "c27cea23",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr_copy = df_cefr.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-81ab8fa7-fab1-498c-a201-42e5f292a658",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6245357,
    "execution_start": 1636627878602,
    "source_hash": "8e05a8b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the map to transform the level column\n",
    "\n",
    "df_cefr.level = df_cefr_copy[\"level\"].map(cefr_level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-63bcb8c7-7a36-4104-97f4-87d62a161869",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 49,
    "execution_start": 1636627878606,
    "source_hash": "467c10dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate with the median\n",
    "\n",
    "df_cefr = df_cefr.groupby(\"word\").agg(\"median\").reset_index()\n",
    "\n",
    "display(df_cefr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-58c7cde6-410a-4e31-946c-9402e366c510",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Finally we change the index to the word itself to facilitate searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-018ebaca-0490-47d0-ad3a-42c9a85fcd47",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1636627887076,
    "source_hash": "fc853559",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cefr.index = df_cefr.word\n",
    "df_cefr = df_cefr.filter(items=[\"level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-e8e262d5-218b-48df-901a-82b3969937ee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we can finally apply this to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-e4c312e2-ae60-48e5-9abd-6145d5f8c347",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1079,
    "execution_start": 1636627895974,
    "source_hash": "819e084f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_data = pd.read_csv(CEFR_CLEAN_CSV_IN, index_col=\"word\")\n",
    "cefr_data.loc[\"he\"].loc[\"level\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-c34a4a1e-f0ed-4ae3-a3db-51ee758d95ea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636627897052,
    "source_hash": "3be90380",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cefr_level_hist(dt_text: str, hc_text: str):\n",
    "\n",
    "    cefr_data = pd.read_csv(CEFR_CLEAN_CSV_IN, index_col=\"word\")\n",
    "    cefr_data.loc[\"he\"].loc[\"level\"]\n",
    "    \n",
    "    level = lambda w: cefr_data.loc[w].loc[\"level\"] if w in cefr_data.index else 0\n",
    "\n",
    "    dt_level = list(map(level, dt_text.split()))\n",
    "    hc_level = list(map(level, hc_text.split()))\n",
    "\n",
    "    kwargs = {\n",
    "        \"x\"    : [hc_level, dt_level],\n",
    "        \"label\" : [\"Hillary Clinton\", \"Donald Trump\"],\n",
    "        \"color\" : [\"b\", \"r\"],\n",
    "        \"alpha\" : 0.4,\n",
    "        \"bins\" : range(0, 7),\n",
    "        \"density\" : True,\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    plt.hist(**kwargs)\n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram of English level of words\", fontsize=16)\n",
    "    plt.xlabel(\"English level\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "    plt.xticks(range(0, 7), [\"NA\", \"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-76a016f3-dcbd-47e9-b61d-aad52cc9dabc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8380,
    "execution_start": 1636627988679,
    "source_hash": "320fd282",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cefr_level_hist(dt_text.lower(), hc_text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-2010d14a-5465-46c1-9a07-70de3a7b317d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There seems to be little difference in terms of language levels, but note how many words were not accounted for; this is due in part to limitations of our CEFR dataset, and also to the fact that some words appear in text in different forms (e.g. a verb conjugated to the past tense). This particular issue will be remedied by \"stemming\" the words to reduce them to their \"base\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-e516fc39-2203-4228-a596-a70c710fcdff",
    "deepnote_cell_type": "text-cell-h3",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-641a5cca-81ab-414f-a4d9-a15f612ec7d0",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "- Who is are Trump and Clinton most confused with? Using the other speaker attributions in the quotas list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-d81c6fba-39b1-4038-b276-31424bf154d8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### b) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-7901461f-b123-4ddd-93b0-8b7a56ba60df",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1636628014104,
    "source_hash": "6be9ac84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_sample.groupby(['speaker']).count().sort_values('quotation', ascending=False)['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-1d80104a-8fbb-45d5-869b-9acc243f5c73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Using NLTK’s Pre-Trained Sentiment Analyzer\n",
    "We use NLTK VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-8c09ef1c-e0b0-4b8c-bb12-941f21af8fea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636628096363,
    "source_hash": "c4301c8e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-b14c7f35-80d4-434d-bcdb-0b19407b08bc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1117,
    "execution_start": 1636628104875,
    "source_hash": "297ff81f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_polarity_scores = dt_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "dt_cleaned_sample['polarityScore'] = [score.get('compound') for score in trump_polarity_scores]\n",
    "\n",
    "clinton_polarity_scores = hc_cleaned_sample['quotation'].apply(sia.polarity_scores)\n",
    "hc_cleaned_sample['polarityScore'] = [score.get('compound') for score in clinton_polarity_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topk_quotes(df, sentiment, k):\n",
    "    if (sentiment == \"positive\"):\n",
    "        print(\"Top {} {} quotes:\".format(k, sentiment))\n",
    "        [print(\"\\n[+]\", quote) for quote in df.sort_values('polarityScore', ascending=False)['quotation'].head(k).tolist()]\n",
    "    else:\n",
    "        print(\"Top {} {} quotes:\".format(k, sentiment))\n",
    "        [print(\"\\n[-]\", quote) for quote in df.sort_values('polarityScore', ascending=True)['quotation'].head(k).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-8a5144b1-48ed-4db9-bd4d-b2aec6dda51d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Donald Trump's 3 most positive and negative quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-c8eac542-3bcf-492b-b29f-f9008f466a92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1636628114523,
    "source_hash": "da136360",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_topk_quotes(dt_cleaned_sample, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(dt_cleaned_sample, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00043-2f3e0e22-945f-422b-b7ae-50449805d62a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Hillary Clinton's 3 most positive and negative quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-5c0acf26-1036-4ef7-ad8d-769ab34a37e6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39,
    "execution_start": 1636628122215,
    "source_hash": "dd5a4f6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_topk_quotes(hc_cleaned_sample, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(hc_cleaned_sample, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-a67380e5-aba9-41bc-9b85-1e2eb5609c87",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 889,
    "execution_start": 1636628130647,
    "source_hash": "d0db659a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_df_pos = dt_cleaned_sample[dt_cleaned_sample['polarityScore'] >= 0]\n",
    "trump_df_neg = dt_cleaned_sample[dt_cleaned_sample['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_df_pos['date'], y=trump_df_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_df_neg['date'], y=trump_df_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-98198884-dda8-47de-9e2b-678e1574e10d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 702,
    "execution_start": 1636628140656,
    "source_hash": "3b4cb853",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinton_df_pos = hc_cleaned_sample[hc_cleaned_sample['polarityScore'] >= 0]\n",
    "clinton_df_neg = hc_cleaned_sample[hc_cleaned_sample['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=clinton_df_pos['date'], y=clinton_df_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=clinton_df_neg['date'], y=clinton_df_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Clinton's quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Hillary Clinton and Donald Trumps compound sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate totals\n",
    "dt_tot_quotes_count = len(dt_cleaned_sample)\n",
    "dt_pos_quotes_count = sum(dt_cleaned_sample['polarityScore']>=0.05)\n",
    "dt_neg_quotes_count = sum(dt_cleaned_sample['polarityScore']<=-0.05)\n",
    "dt_neu_quotes_count = sum(np.abs(dt_cleaned_sample['polarityScore'])<0.05)\n",
    "\n",
    "hc_tot_quotes_count = len(hc_cleaned_sample)\n",
    "hc_pos_quotes_count = sum(hc_cleaned_sample['polarityScore']>=0.05)\n",
    "hc_neg_quotes_count = sum(hc_cleaned_sample['polarityScore']<=-0.05)\n",
    "hc_neu_quotes_count = sum(np.abs(hc_cleaned_sample['polarityScore'])<0.05)\n",
    "\n",
    "# summary\n",
    "print()\n",
    "print('                                     Hilary Clinton     |      Donald Trump      ')\n",
    "print('                                ------------------------|------------------------')\n",
    "print('Total number quotes in sample:        {:>4} ({:.1%})     |      {:>4} ({:.1%})'.format(hc_tot_quotes_count, hc_tot_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                               dt_tot_quotes_count, dt_tot_quotes_count/dt_tot_quotes_count))\n",
    "print('Number of negative quotes:            {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_neg_quotes_count, hc_neg_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_neg_quotes_count, dt_neg_quotes_count/dt_tot_quotes_count ))\n",
    "print('Number of neutral quotes:             {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_neu_quotes_count, hc_neu_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_neu_quotes_count, dt_neu_quotes_count/dt_tot_quotes_count))\n",
    "print('Number of positive quotes:            {:>4} ({:.1%})      |      {:>4} ({:.1%})'.format(hc_pos_quotes_count, hc_pos_quotes_count/hc_tot_quotes_count,\n",
    "                                                                                                dt_pos_quotes_count, dt_pos_quotes_count/dt_tot_quotes_count ))\n",
    "\n",
    "print(\"\\n(Positive score > 0.05, Negative score < -0.05, Neutral score: -0.05 < score < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords):\n",
    "    hc_filtered = hc_sample[hc_sample['quotation'].str.contains(topic_keywords, case=False)]\n",
    "    dt_filtered = dt_sample[dt_sample['quotation'].str.contains(topic_keywords, case=False)]\n",
    "    \n",
    "    hc_dt_filtered = pd.DataFrame(dict(\n",
    "        candidate = np.concatenate(([\"Hillary Clinton\"]*len(hc_filtered), \n",
    "                                 [\"Donald Trump\"]*len(dt_filtered))), \n",
    "        polarityScore   = np.concatenate((hc_filtered['polarityScore'],\n",
    "                                          dt_filtered['polarityScore'])),\n",
    "        quotation   = np.concatenate((hc_filtered['quotation'],\n",
    "                                          dt_filtered['quotation']))\n",
    "    ))\n",
    "    return hc_dt_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentHistogram(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "    # plot with plotly\n",
    "    fig = px.histogram(hc_dt_filtered, x=\"polarityScore\", color=\"candidate\", histnorm='probability',\n",
    "                       barmode=\"overlay\", nbins=40, marginal=\"box\",\n",
    "                       width=600, height=400,\n",
    "                       log_y=True\n",
    "    )\n",
    "\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Sentiment distribution (topic = '+topic_keywords+')', title_x=0.5,\n",
    "        xaxis_title_text='Sentiment (polarity compound score)',\n",
    "        yaxis_title_text='Number of quotes (log scale)', \n",
    "        # bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "        bargroupgap=0.05, # gap between bars of the same location coordinates\n",
    "    )\n",
    "    fig.update_xaxes(range=[-1, 1])\n",
    "    plot_filename = JEKYLL_PLOTS_PATH + \"sentiment_hist_\" + topic_keywords[:5] + \".html\"\n",
    "    fig.write_html(plot_filename, include_plotlyjs=False, full_html=False, default_width='66%', default_height='66%')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotesCountPerSpeaker(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "    quote_counts = hc_dt_filtered.groupby(['candidate']).size().reset_index(name='counts')\n",
    "    \n",
    "    # sort to have Clinton on the left and Trump on the right\n",
    "    quote_counts.sort_values('candidate', ascending=False, inplace=True)\n",
    "\n",
    "    fig = px.bar(quote_counts, x=\"candidate\", y=\"counts\",\n",
    "                 color='candidate',\n",
    "                 color_discrete_map={\n",
    "                    'Hillary Clinton': px.colors.qualitative.Plotly[0],\n",
    "                    'Donald Trump': px.colors.qualitative.Plotly[1],\n",
    "                 },\n",
    "                width=600, height=400,\n",
    "                text=\"counts\"\n",
    "                )\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Total number of quotes (topic = '+topic_keywords+')', title_x=0.5,\n",
    "        xaxis_title_text='Candidate',\n",
    "        yaxis_title_text='Number of quotes', \n",
    "    )\n",
    "    plot_filename = JEKYLL_PLOTS_PATH + \"quotes_count_per_speaker_\" + topic_keywords[:5] + \".html\"\n",
    "    fig.write_html(plot_filename, include_plotlyjs=False, full_html=False, default_width='66%', default_height='66%')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentClassifier(hc_sample, dt_sample, topic_keywords): \n",
    "    hc_dt_filtered = filter_n_concatenate_dfs(hc_sample, dt_sample, topic_keywords)\n",
    "\n",
    "    # create a list of our conditions\n",
    "    sentiment_conditions = [\n",
    "        (hc_dt_filtered['polarityScore'] <= -0.05),\n",
    "        (np.abs(hc_dt_filtered['polarityScore']) < 0.05),\n",
    "        (hc_dt_filtered['polarityScore'] >= 0.05)\n",
    "    ]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['Negative &#128545;<br>(score < -0.05)', 'Neutral &#128528; <br>(-0.05 < score < 0.05)', 'Positive &#128515;<br>(score > 0.05)']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    hc_dt_filtered['sentiment'] = np.select(sentiment_conditions, values)\n",
    "\n",
    "    # aggregate our dataframe by sentiment and candidate\n",
    "    df_sentiment_agg = hc_dt_filtered.groupby(['sentiment', 'candidate']).size().reset_index(name='counts')\n",
    "\n",
    "    # NORMALIZATION\n",
    "    # compute total number of quotes per candidate\n",
    "    quote_counts = hc_dt_filtered.groupby(['candidate']).size()\n",
    "    hc_quote_counts = quote_counts['Hillary Clinton']\n",
    "    dt_quote_counts = quote_counts['Donald Trump']\n",
    "\n",
    "    # divide by total number of quotes per candidate to get percentage\n",
    "    df_sentiment_agg['counts_pct'] = 0\n",
    "    df_sentiment_agg['counts_pct'] = np.where(df_sentiment_agg['candidate'] == 'Donald Trump', df_sentiment_agg['counts']/dt_quote_counts, df_sentiment_agg['counts_pct'])\n",
    "    df_sentiment_agg['counts_pct'] = np.where(df_sentiment_agg['candidate'] == 'Hillary Clinton', df_sentiment_agg['counts']/hc_quote_counts, df_sentiment_agg['counts_pct'])\n",
    "\n",
    "    # sort to have Clinton on the left and Trump on the right\n",
    "    df_sentiment_agg.sort_values('candidate', ascending=False, inplace=True)\n",
    "\n",
    "    fig = px.bar(df_sentiment_agg, x=\"sentiment\", y=\"counts_pct\",\n",
    "                 color='candidate', barmode='group',\n",
    "                 color_discrete_map={\n",
    "                    'Hillary Clinton': px.colors.qualitative.Plotly[0],\n",
    "                    'Donald Trump': px.colors.qualitative.Plotly[1],\n",
    "                 },\n",
    "                 width=600, height=400,\n",
    "                 text=\"counts_pct\",\n",
    "                 hover_data={\n",
    "                    'candidate': True, \n",
    "                    'sentiment': True, \n",
    "                    'counts_pct': False, \n",
    "                    'counts': True, \n",
    "                 }\n",
    "    )\n",
    "\n",
    "    # customize font and legend orientation & position\n",
    "    fig.update_layout( \n",
    "        font_family=\"Rockwell\",\n",
    "        legend=dict(\n",
    "            title=None, orientation=\"h\", y=1, yanchor=\"bottom\", x=0.5, xanchor=\"center\"\n",
    "        ),\n",
    "        title_text='Quotes sentiment classification (topic = '+topic_keywords+')', title_x=0.5,\n",
    "        xaxis_title_text='Sentiment',\n",
    "        yaxis_title_text='% of quotes per candidate'\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(texttemplate='%{text:.0%}', textposition='inside')\n",
    "    plot_filename = JEKYLL_PLOTS_PATH + \"sentiment_class_\" + topic_keywords[:5] + \".html\"\n",
    "    fig.write_html(plot_filename, include_plotlyjs=False, full_html=False, default_width='66%', default_height='66%')\n",
    "    fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = ''\n",
    "quotesCountPerSpeaker(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentClassifier(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentHistogram(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'economy'\n",
    "quotesCountPerSpeaker(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentClassifier(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentHistogram(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "\n",
    "hc_dt_filtered = filter_n_concatenate_dfs(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "print_topk_quotes(hc_dt_filtered, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(hc_dt_filtered, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'immigration|border'\n",
    "quotesCountPerSpeaker(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentClassifier(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentHistogram(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "\n",
    "hc_dt_filtered = filter_n_concatenate_dfs(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "print_topk_quotes(hc_dt_filtered, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(hc_dt_filtered, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'obamacare'\n",
    "quotesCountPerSpeaker(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentClassifier(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentHistogram(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "\n",
    "hc_dt_filtered = filter_n_concatenate_dfs(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "print_topk_quotes(hc_dt_filtered, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(hc_dt_filtered, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = 'abortion'\n",
    "quotesCountPerSpeaker(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentClassifier(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "sentimentHistogram(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "\n",
    "hc_dt_filtered = filter_n_concatenate_dfs(hc_cleaned_sample, dt_cleaned_sample, topic_keywords)\n",
    "print_topk_quotes(hc_dt_filtered, \"positive\", 3)\n",
    "print(\"\\n---\\n\")\n",
    "print_topk_quotes(hc_dt_filtered, \"negative\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00067-4060b568-755a-4b12-b428-8ef964b364a6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Sentiment analysis on the target of quotes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, Trump calling Clinton \"Crooked Hillary\" would be a negative statement about Presidential candidate Clinton. This will be done for a number of politicians, including: Hillary Clinton, Nancy Pelosi, Barack Obama, Bernie Sanders, Elizabeth Warren, Ted Cruz, Mike Pence, and Mitch McConnell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00068-630091d6-988f-4d0f-b56f-ebdaa90a1ad8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1636628202255,
    "source_hash": "7ab19ee2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_df = sample.loc[sample['speaker'] == 'Donald Trump']\n",
    "clinton_df = sample.loc[sample['speaker'] == 'Hillary Clinton']\n",
    "\n",
    "trump_on_clinton = trump_df[trump_df['quotation'].str.contains('clinton|hillary', case=False)]\n",
    "trump_on_obama = trump_df[trump_df['quotation'].str.contains('obama|barack', case=False)]\n",
    "trump_on_sanders = trump_df[trump_df['quotation'].str.contains('bernie|sanders', case=False)]\n",
    "\n",
    "clinton_on_trump = clinton_df[clinton_df['quotation'].str.contains('trump|donald', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00069-c34cd4a1-af5a-49a4-8152-7de63b2f3811",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 71,
    "execution_start": 1636628211016,
    "source_hash": "952ab1a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_clinton_PS = trump_on_clinton['quotation'].apply(sia.polarity_scores)\n",
    "trump_on_clinton['polarityScore'] = [score.get('compound') for score in trump_on_clinton_PS]\n",
    "\n",
    "trump_on_obama_PS = trump_on_obama['quotation'].apply(sia.polarity_scores)\n",
    "trump_on_obama['polarityScore'] = [score.get('compound') for score in trump_on_obama_PS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00070-17faff10-d143-4775-bf17-92941d690a18",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 786,
    "execution_start": 1636628233584,
    "source_hash": "a6243f79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_clinton_pos = trump_on_clinton[trump_on_clinton['polarityScore'] >= 0]\n",
    "trump_on_clinton_neg = trump_on_clinton[trump_on_clinton['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_on_clinton_pos['date'], y=trump_on_clinton_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_on_clinton_neg['date'], y=trump_on_clinton_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's Hillary-targeted quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00071-28c2b03b-740e-48bc-8007-0e938deb775c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 713,
    "execution_start": 1636628288940,
    "source_hash": "28ea1bcd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trump_on_obama_pos = trump_on_obama[trump_on_obama['polarityScore'] >= 0]\n",
    "trump_on_obama_neg = trump_on_obama[trump_on_obama['polarityScore'] < 0]\n",
    "\n",
    "plt.scatter(x=trump_on_obama_pos['date'], y=trump_on_obama_pos['polarityScore'], color='g', alpha=0.3, label=\"positive\")\n",
    "plt.scatter(x=trump_on_obama_neg['date'], y=trump_on_obama_neg['polarityScore'], color='r', alpha=0.3, label=\"negative\")\n",
    "\n",
    "plt.title(\"Trump's Obama-targeted quotes compound polarity scores by date in 2016\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00072-23bb561b-cc74-4903-b22e-f035b6895e8f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Perhaps a next step would be to compare this with a baseline ratio of positive/negative words. For instance, maybe on average people say 70% positive things and 30% negative. Knowing this could help compare results with Trump and Clinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large part of analyzing someone's speech is not only about what they say, but how they say it. People reveal who they are through their own words. Now besides content words such as nouns, regular & action verbs, and modifiers (adjectives and adverbs), there is a separate class of words called style or function words that, on their own, do not signify anything.\n",
    "\n",
    "However, it turns out these function words are very good at indicating the current emotion of the speaker as well as how they think since they are processed differently in the brain and their use follows a power law in most languages. For instance, when someone is depressed, they will use the pronoun \"I\" more frequently. There are also gender differences when it comes to function words. Women use more first-person words such as \"I\" or \"we\" whereas men prefer to use articles like \"a\" and \"the\".\n",
    "\n",
    "How do these differences fare between Hillary Clinton and Donald Trump? Do they talk more about themselves with \"I\" or \"me\" or about others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting number of most frequently used pronouns in the English language by candidate\n",
    "\n",
    "freq_pronouns = ['it','I','you','he','they','we','she','who','them','me']\n",
    "trump_pronouns_spc, clinton_pronouns_spc = [],[]\n",
    "trump_I_count, clinton_I_count, trump_who_count, clinton_who_count = [],[],[],[]\n",
    "trump_nb_pronouns, clinton_nb_pronouns, trump_spc_count, clinton_spc_count = 0,0,0,0\n",
    "\n",
    "for k in freq_pronouns:\n",
    "    for i in range(len(trump_df)):\n",
    "        count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(k), trump_df['quotation'].iloc[i]))\n",
    "        trump_nb_pronouns += count\n",
    "        trump_spc_count += count\n",
    "        if k == 'I':\n",
    "            trump_I_count.append(count)\n",
    "        if k == 'who':\n",
    "            trump_who_count.append(count)\n",
    "    trump_pronouns_spc.append(trump_spc_count)\n",
    "    trump_spc_count = 0\n",
    "\n",
    "for k in freq_pronouns:\n",
    "    for i in range(len(clinton_df)):\n",
    "        count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(k), clinton_df['quotation'].iloc[i]))\n",
    "        clinton_nb_pronouns += count\n",
    "        clinton_spc_count += count\n",
    "        if k == 'I':\n",
    "            clinton_I_count.append(count)\n",
    "        if k == 'who':\n",
    "            clinton_who_count.append(count)\n",
    "    clinton_pronouns_spc.append(clinton_spc_count)\n",
    "    clinton_spc_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure().update_xaxes(categoryorder = \"total descending\")\n",
    "fig.add_trace(go.Bar(\n",
    "    x=freq_pronouns,\n",
    "    y=scaled_clinton_pronouns,\n",
    "    name='Clinton pronoun count',\n",
    "    marker_color='blue'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=freq_pronouns,\n",
    "    y=scaled_trump_pronouns,\n",
    "    name='Trump pronoun count',\n",
    "    marker_color='red'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting finding relates to the social class differences in language patterns. People from higher social classes tend to use more articles and prepositions compared to their lower class counterparts which use more pronouns and auxiliary verbs. This difference is known to be statistically significant.\n",
    "\n",
    "Does Donald Trump target the lower and middle classes better? The heartland of America? It certainly helped Trump in his case, since listeners feel closer to the speaker when the word \"I\" is used more often, even though political advisors usually suggest to use more \"we\" which unfortunately creates the opposite effect, such as during John Kerry's 2004 Presidential run. However, it is very likely that knowing this wouldn't have changed the outcome in 2004 nor in 2016. Language is a powerful reflection of a person's personality and character but does not change a person on its own.\n",
    "\n",
    "This is in fact a counterintuitive finding as well since, as a man, Donald Trump would be statistically much more likely to use more articles and nouns and less likely to use pronouns than Hillary Clinton, his female adversary. Donald Trump's high social status also does not account for this pronoun use by the Republican candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting average sentiment in sentences with select pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_sentiment_pronouns, clinton_sentiment_pronouns = [],[]\n",
    "\n",
    "for i in freq_pronouns:\n",
    "    temp_trump = dt_cleaned_sample[dt_cleaned_sample['quotation'].str.contains(i)]\n",
    "    trump_sentiment_pronouns.append(temp_trump['polarityScore'].mean())\n",
    "    temp_clinton = hc_cleaned_sample[hc_cleaned_sample['quotation'].str.contains(i)]\n",
    "    clinton_sentiment_pronouns.append(temp_clinton['polarityScore'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure().update_xaxes(categoryorder = \"total descending\")\n",
    "fig.add_trace(go.Bar(\n",
    "    x=freq_pronouns,\n",
    "    y=clinton_sentiment_pronouns,\n",
    "    name='Clinton pronoun count',\n",
    "    marker_color='blue'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=freq_pronouns,\n",
    "    y=trump_sentiment_pronouns,\n",
    "    name='Trump pronoun count',\n",
    "    marker_color='red'\n",
    "))\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing random negative Trump quote containing \"who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "trump_who = np.array(trump_who_count)\n",
    "trump_who_indices = trump_who.nonzero()\n",
    "\n",
    "random_quote = random.randint(0, len(trump_who_indices[0]))\n",
    "trump_df['quotation'].iloc[trump_who_indices[0][random_quote]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Crooked Hillary\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trump famously popularized the harsh phrase \"Crooked Hillary\" to denote the dishonesty that he perceived from his opponent. But was there some basis for this statement? It turns out, deception can also be captured by language to some degree. Most people, when telling the truth about an important situation, will use more often the pronoun \"I\" (single best predictor of a person's honesty*) as well as more negative emotion. However, in our case, Clinton used both less \"I\" and less negative emotion than Trump for most pronouns. This gives some evidence for the validity of Trump's nickname for Clinton, cruel as it is.\n",
    "\n",
    "*Taken from Pennebaker, James W. The Secret Life of Pronouns. Bloomsbury Publishing."
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d1c70e77-63ff-44c4-badd-242127365f55",
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
